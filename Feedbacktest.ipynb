{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from db_create import CargaDeArchivos\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from Tools.Feedback_tool import collect_and_store_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "a= CargaDeArchivos()\n",
    "a.run()\n",
    "db_conn= a.conn\n",
    "login(token=\"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\")\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the workflow, including the question, schema, database connection,\n",
    "    relevance, SQL query, query result, and other metadata.\n",
    "    \"\"\"\n",
    "    original_question: str\n",
    "    questions: List[str] = []\n",
    "    db_conn: None\n",
    "    query_dfs: List[pd.DataFrame] = []\n",
    "    relevance: str\n",
    "    sql_querys: List[str] = []\n",
    "    query_results: List[str] = []\n",
    "    sql_error: List[bool]= []\n",
    "    final_answer: str\n",
    "    attempts: int\n",
    "    chat_history: List[str] = []\n",
    "    context_length: int = 0\n",
    "    use_case: str\n",
    "    tokenizer: any\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auxiliary functions\n",
    "def count_tokens(text: str, tokenizer) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given text using the provided tokenizer.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "def identify_question_type(q: str) -> str:\n",
    "    q = q.lower()\n",
    "    if any(w in q for w in [\"average\", \"mean\", \"duration\", \"time taken\", \"how long\"]):\n",
    "        return \"average\"\n",
    "    if any(w in q for w in [\"distribution\", \"frequency\", \"histogram\"]):\n",
    "        return \"distribution\"\n",
    "    if any(w in q for w in [\"trend\", \"over time\", \"change\", \"evolution\"]):\n",
    "        return \"trend\"\n",
    "    if any(w in q for w in [\"most\", \"top\", \"highest\", \"least\", \"lowest\", \"compare\"]):\n",
    "        return \"ranking\"\n",
    "    return \"general\"\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame, question_type: str) -> str:\n",
    "    summary = \"\"\n",
    "\n",
    "    if df.empty:\n",
    "        return \"⚠️ No data to summarize.\"\n",
    "\n",
    "    if question_type == \"average\":\n",
    "        numeric_cols = df.select_dtypes(include=\"number\")\n",
    "        if not numeric_cols.empty:\n",
    "            summary += numeric_cols.mean().to_frame(\"mean\").T.to_string()\n",
    "        else:\n",
    "            summary += \"ℹ️ No numeric columns to compute averages.\"\n",
    "    elif question_type == \"distribution\":\n",
    "        for col in df.select_dtypes(include=[\"object\", \"category\"]):\n",
    "            dist = df[col].value_counts(normalize=True).head(3)\n",
    "            summary += f\"\\n- {col}: {dist.to_dict()}\"\n",
    "    elif question_type == \"trend\":\n",
    "        time_cols = [col for col in df.columns if \"date\" in col.lower() or \"time\" in col.lower()]\n",
    "        if time_cols:\n",
    "            col = time_cols[0]\n",
    "            df_sorted = df.sort_values(by=col)\n",
    "            summary += f\"Sample over time ({col}):\\n\"\n",
    "            summary += df_sorted[[col]].head(5).to_string(index=False)\n",
    "        else:\n",
    "            summary += \"ℹ️ No time-related column found to show trend.\"\n",
    "    elif question_type == \"ranking\":\n",
    "        numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "        if len(numeric_cols) >= 1:\n",
    "            col = numeric_cols[0]\n",
    "            top = df.nlargest(3, col)[[col]].to_string(index=False)\n",
    "            summary += f\"Top 3 rows by {col}:\\n{top}\"\n",
    "        else:\n",
    "            summary += \"ℹ️ No numeric column found for ranking.\"\n",
    "    else:  # General fallback\n",
    "        summary += df.describe(include='all').to_string()\n",
    "    return summary\n",
    "\n",
    "def relevant_entries(chat_history_entries):\n",
    "    \"\"\"\n",
    "    Filters and retrieves the last 3 relevant user questions and their responses in correct order.\n",
    "\n",
    "    Args:\n",
    "        chat_history_entries (list): Full chat history.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the last 3 relevant interactions in correct order.\n",
    "    \"\"\"\n",
    "    relevant_pairs = []\n",
    "    found_count = 0\n",
    "    idx = len(chat_history_entries) - 1\n",
    "\n",
    "    while idx >= 0:\n",
    "        entry = chat_history_entries[idx]\n",
    "\n",
    "        if \"[Relevance: relevant]\" in entry:\n",
    "            user_question = entry  # Store user question\n",
    "\n",
    "            # Look for sOFIa's response **before** storing the question\n",
    "            response_idx = idx + 1  \n",
    "            if response_idx < len(chat_history_entries) and chat_history_entries[response_idx].startswith(\"sOFIa:\"):\n",
    "                sofia_response = chat_history_entries[response_idx]\n",
    "                relevant_pairs.append((user_question, sofia_response))  # Save as a pair\n",
    "                found_count += 1\n",
    "\n",
    "            if found_count >= 3:\n",
    "                break  # Stop after collecting 3 pairs\n",
    "\n",
    "        idx -= 1  # Move backwards in history\n",
    "\n",
    "    # Reverse to maintain chronological order and format correctly\n",
    "    formatted_history = \"\\n\".join(f\"{q}\\n{a}\" for q, a in reversed(relevant_pairs))\n",
    "    return formatted_history\n",
    "\n",
    "def non_relevant_entries(chat_history_entries):\n",
    "    \"\"\"\n",
    "    Filters and retrieves the last 3 non-relevant user questions and their responses in correct order.\n",
    "\n",
    "    Args:\n",
    "        chat_history_entries (list): Full chat history.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the last 3 non-relevant interactions in correct order.\n",
    "    \"\"\"\n",
    "    non_relevant_pairs = []\n",
    "    found_count = 0\n",
    "    idx = len(chat_history_entries) - 1\n",
    "\n",
    "    while idx >= 0:\n",
    "        entry = chat_history_entries[idx]\n",
    "\n",
    "        if \"[Relevance: not_relevant]\" in entry:\n",
    "            user_question = entry  # Store user question\n",
    "\n",
    "            # Look for sOFIa's response **before** storing the question\n",
    "            response_idx = idx + 1  \n",
    "            if response_idx < len(chat_history_entries) and chat_history_entries[response_idx].startswith(\"sOFIa:\"):\n",
    "                sofia_response = chat_history_entries[response_idx]\n",
    "                non_relevant_pairs.append((user_question, sofia_response))  # Save as a pair\n",
    "                found_count += 1\n",
    "\n",
    "            if found_count >= 3:\n",
    "                break  # Stop after collecting 3 pairs\n",
    "\n",
    "        idx -= 1  # Move backwards in history\n",
    "\n",
    "    # Reverse to maintain chronological order and format correctly\n",
    "    formatted_history = \"\\n\".join(f\"{q}\\n{a}\" for q, a in reversed(non_relevant_pairs))\n",
    "    return formatted_history\n",
    "\n",
    "p1_p= \"\"\" \n",
    "    You are an SQL assistant specialized in DuckDB. Your task is to generate accurate SQL queries based on natural language questions, following the schema and rules below.\n",
    "\n",
    "    ### Schema (Aliased)\n",
    "\n",
    "    - **cases**  \n",
    "    - id (VARCHAR): Case identifier (PK)  \n",
    "    - avg_time (DOUBLE): Total duration (sec) from start to closure  \n",
    "    - type, branch, ramo, broker, state, client, creator (VARCHAR): Case metadata  \n",
    "    - value (BIGINT): Insurance amount  \n",
    "    - approved (BOOLEAN): Approval status  \n",
    "    - case_order_date, case_estimated_delivery, case_delivery (TIMESTAMP): Case timestamps  \n",
    "    - case_employee_id, case_branch, case_supplier (VARCHAR): Case-specific information  \n",
    "    - case_number_of_items, case_ft_items (INTEGER): Case item details  \n",
    "    - case_total_price (DOUBLE): Case total price\n",
    "\n",
    "    - **activities**  \n",
    "    - id (BIGINT): Activity identifier (PK)  \n",
    "    - case_id (VARCHAR): Case ID (FK → cases.id)  \n",
    "    - timestamp (TIMESTAMP): Activity timestamp  \n",
    "    - name (VARCHAR): Activity name  \n",
    "    - case_index (BIGINT): Alias of id  \n",
    "    - tpt (DOUBLE): Duration of the activity in seconds  \n",
    "    - user, user_type (VARCHAR): User-related info  \n",
    "    - automatic, rework (BOOLEAN): Activity flags  \n",
    "    - case_order_date (TIMESTAMP), case_employee_id (VARCHAR), case_branch (VARCHAR), case_supplier (VARCHAR): Case-related data  \n",
    "    - case_avg_time (DOUBLE): Average time for the case  \n",
    "    - case_on_time, case_in_full (BOOLEAN): Delivery status flags  \n",
    "    - case_number_of_items, case_ft_items (INTEGER): Case item counts  \n",
    "    - case_total_price (DOUBLE): Case total price  \n",
    "    - case_estimated_delivery, case_delivery (TIMESTAMP): Delivery-related timestamps\n",
    "\n",
    "    - **variants**  \n",
    "    - id (BIGINT): Variant ID (PK for path)  \n",
    "    - activities (VARCHAR[]): Ordered activity names for this path  \n",
    "    - cases (VARCHAR[]): IDs of cases that followed this path (→ cases.id)  \n",
    "    - number_cases (BIGINT): Total cases following this variant  \n",
    "    - percentage (DOUBLE): Percentage of total cases  \n",
    "    - avg_time (DOUBLE): Avg duration (sec) across cases in this variant\n",
    "\n",
    "    ### Query Guidelines\n",
    "\n",
    "    1. Always reference columns with aliases (e.g., c.id, a.case_id)\n",
    "    2. Use `UNNEST()` in the `FROM` clause to access list fields like v.activities or v.cases. Do not use `UNNEST()` inside expressions like `= ANY(...)`.\n",
    "    3. When comparing list values (e.g., activity names), first `UNNEST()` the list in a subquery or CTE, then use direct comparison with `TRIM(...)`.\n",
    "    4. Use `TRIM()` for comparing activity names (e.g., TRIM(a.name) = TRIM(...))\n",
    "    5. Avoid unnecessary joins or full scans when possible\n",
    "    6. Convert time differences with `EXTRACT(EPOCH FROM ...)`\n",
    "    7. Include all non-aggregated columns in `GROUP BY`\n",
    "\n",
    "    ### Variant Comparison Rules\n",
    "\n",
    "    - **Most Frequent Path**:  \n",
    "    Get the variant with the max number_cases:  \n",
    "    `SELECT * FROM variants WHERE number_cases = (SELECT MAX(number_cases) FROM variants)`\n",
    "\n",
    "    - **Variant Durations**:  \n",
    "    Use `avg_time` from `variants` for variant-level durations. Avoid recomputing durations from activity timestamps unless explicitly requested.\n",
    "\n",
    "    - **Deviations**:  \n",
    "    All variants with a different `id` from the most frequent one are deviations.  \n",
    "    When asked for deviation point, just retrieve the full list of activities from the most frequent variant and compare with the other variants.\n",
    "\n",
    "    - **Activity Durations Along Most Frequent Path**:  \n",
    "    1. Extract activities from the most frequent variant using `UNNEST(activities)` in the `FROM` clause.\n",
    "    2. Join with the `activities` table on trimmed name values.\n",
    "    3. Group by activity name and compute average `tpt`.\n",
    "\n",
    "    ### Common Pitfall Corrections\n",
    "\n",
    "    - Never use `UNNEST()` inside `= ANY(...)`. Instead, `UNNEST` in a `FROM` clause or CTE, then join or filter.\n",
    "    - Avoid using `> ALL(...)` for comparisons. Use `ORDER BY ... LIMIT 1` or `= (SELECT MAX(...))`.\n",
    "    - When filtering branches or groups with the highest average, use subqueries like:\n",
    "\n",
    "        ```\n",
    "        SELECT branch\n",
    "        FROM cases\n",
    "        WHERE approved = TRUE\n",
    "        GROUP BY branch\n",
    "        ORDER BY AVG(value) DESC\n",
    "        LIMIT 1\n",
    "        ```\n",
    "\n",
    "    - For aggregated stats over filtered groups (e.g., top branches), prefer subqueries or joins with `IN` from pre-identified sets.\n",
    "    - If no data matches a filter, return `NULL` instead of failing or using over-restrictive filters.\n",
    "    - When detecting repeated activities on the same day, use:\n",
    "\n",
    "        ```\n",
    "        GROUP BY a.case_id, DATE_TRUNC('day', a.timestamp)\n",
    "        HAVING COUNT(*) > 1\n",
    "        ```\n",
    "\n",
    "        Avoid unnecessary joins with `GENERATE_SERIES`.\n",
    "\n",
    "    ### Output\n",
    "\n",
    "    - Return **only** the SQL query. No markdown, no tags, no explanation.\n",
    "    - Never guess values. Always infer based on the data and schema above.\n",
    "    \"\"\"\n",
    "p2_p= \"\"\"### Database Schema\n",
    "\n",
    "                - **cases**  \n",
    "        - id (VARCHAR): Case identifier (PK)  \n",
    "        - avg_time (DOUBLE): Total duration (sec) from start to closure  \n",
    "        - type, branch, ramo, broker, state, client, creator (VARCHAR): Case metadata  \n",
    "        - value (BIGINT): Insurance amount  \n",
    "        - approved (BOOLEAN): Approval status  \n",
    "        - case_order_date, case_estimated_delivery, case_delivery (TIMESTAMP): Case timestamps  \n",
    "        - case_employee_id, case_branch, case_supplier (VARCHAR): Case-specific information  \n",
    "        - case_number_of_items, case_ft_items (INTEGER): Case item details  \n",
    "        - case_total_price (DOUBLE): Case total price\n",
    "\n",
    "        - **activities**  \n",
    "        - id (BIGINT): Activity identifier (PK)  \n",
    "        - case_id (VARCHAR): Case ID (FK → cases.id)  \n",
    "        - timestamp (TIMESTAMP): Activity timestamp  \n",
    "        - name (VARCHAR): Activity name  \n",
    "        - case_index (BIGINT): Alias of id  \n",
    "        - tpt (DOUBLE): Duration of the activity in seconds  \n",
    "        - user, user_type (VARCHAR): User-related info  \n",
    "        - automatic, rework (BOOLEAN): Activity flags  \n",
    "        - case_order_date (TIMESTAMP), case_employee_id (VARCHAR), case_branch (VARCHAR), case_supplier (VARCHAR): Case-related data  \n",
    "        - case_avg_time (DOUBLE): Average time for the case  \n",
    "        - case_on_time, case_in_full (BOOLEAN): Delivery status flags  \n",
    "        - case_number_of_items, case_ft_items (INTEGER): Case item counts  \n",
    "        - case_total_price (DOUBLE): Case total price  \n",
    "        - case_estimated_delivery, case_delivery (TIMESTAMP): Delivery-related timestamps\n",
    "\n",
    "        - **variants**  \n",
    "        - id (BIGINT): Variant ID (PK for path)  \n",
    "        - activities (VARCHAR[]): Ordered activity names for this path  \n",
    "        - cases (VARCHAR[]): IDs of cases that followed this path (→ cases.id)  \n",
    "        - number_cases (BIGINT): Total cases following this variant  \n",
    "        - percentage (DOUBLE): Percentage of total cases  \n",
    "        - avg_time (DOUBLE): Avg duration (sec) across cases in this variant\n",
    "\n",
    "            **Relations:**\n",
    "            - \"variants\".\"cases\" references \"cases\".\"id\", meaning each variant is followed by multiple cases.\n",
    "            - \"variants\".\"activities\" corresponds to the ordered \"activities\".\"name\" values for those cases.\n",
    "            \"\"\"\n",
    "p1_i= \"\"\"\n",
    "        You are an SQL assistant specialized in DuckDB. Your task is to generate accurate SQL queries based on natural language questions, following the schema and rules below.\n",
    "\n",
    "        ### Schema (Aliased)\n",
    "\n",
    "            - **grouped (g)**  \n",
    "            - group_id (VARCHAR): Unique identifier for each group (PK)  \n",
    "            - amount_overpaid (BIGINT): Total overpaid amount for the group  \n",
    "            - itemCount (BIGINT): Number of items in the group  \n",
    "            - date (VARCHAR): Date of the group  \n",
    "            - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "            - open (BOOLEAN): Status of the group (open or closed)  \n",
    "            - confidence (VARCHAR): Confidence level for detecting the pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "            - items (STRUCT[]): Array of items within the group, each containing:\n",
    "                - **id (INTEGER)**: Item identifier (FK → invoices.id)\n",
    "                - **case (STRUCT)**: Contains case details, such as:\n",
    "                    - id (VARCHAR): Case identifier  \n",
    "                    - order_date (VARCHAR): Order date for the case  \n",
    "                    - employee_id (VARCHAR): Employee ID handling the case  \n",
    "                    - branch (VARCHAR): Branch handling the case  \n",
    "                    - supplier (VARCHAR): Supplier associated with the case  \n",
    "                    - avg_time (DOUBLE): Average time for the case  \n",
    "                    - estimated_delivery (VARCHAR): Estimated delivery date for the case  \n",
    "                    - delivery (VARCHAR): Actual delivery date for the case  \n",
    "                    - on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "                    - in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "                    - number_of_items (INTEGER): Number of items in the case  \n",
    "                    - ft_items (INTEGER): Number of full-time items in the case  \n",
    "                    - total_price (INTEGER): Total price of the case  \n",
    "                - date (VARCHAR): Date of the item  \n",
    "                - unit_price (VARCHAR): Unit price of the item  \n",
    "                - quantity (INTEGER): Quantity of the item  \n",
    "                - value (VARCHAR): Value of the item  \n",
    "                - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'  \n",
    "                - open (BOOLEAN): Status of the item (open or closed)  \n",
    "                - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "                - confidence (VARCHAR): Confidence level for the item’s pattern (e.g., \"high\", \"medium\", \"low\")  \n",
    "                - description (VARCHAR): Description of the item  \n",
    "                - payment_method (VARCHAR): Payment method used for the item  \n",
    "                - pay_date (VARCHAR): Payment date of the item  \n",
    "                - special_instructions (VARCHAR): Special instructions for the item  \n",
    "                - accuracy (INTEGER): Accuracy of the item’s data matching\n",
    "\n",
    "            - **invoices (i)**  \n",
    "            - id (BIGINT): Invoice identifier (PK)  \n",
    "            - date (TIMESTAMP_NS): Date and time the invoice was issued  \n",
    "            - unit_price (VARCHAR): Unit price of the item in the invoice  \n",
    "            - quantity (BIGINT): Number of items in the invoice  \n",
    "            - value (VARCHAR): Total value of the invoice  \n",
    "            - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "            - open (BOOLEAN): Status of the invoice (open or closed)  \n",
    "            - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "            - confidence (VARCHAR): Confidence level for the invoice's pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "            - description (VARCHAR): Description of the invoice  \n",
    "            - payment_method (VARCHAR): Method used for payment  \n",
    "            - pay_date (TIMESTAMP_NS): Date and time the invoice was paid  \n",
    "            - special_instructions (VARCHAR): Any special instructions for the invoice  \n",
    "            - accuracy (BIGINT): Accuracy of the invoice's data matching  \n",
    "            - case_id (VARCHAR): Case identifier associated with the invoice  \n",
    "            - case_order_date (TIMESTAMP_NS): Order date of the case  \n",
    "            - case_employee_id (VARCHAR): Employee associated with the case  \n",
    "            - case_branch (VARCHAR): Branch where the case was handled  \n",
    "            - case_supplier (VARCHAR): Supplier associated with the case  \n",
    "            - case_avg_time (DOUBLE): Average time for the case  \n",
    "            - case_estimated_delivery (TIMESTAMP_NS): Estimated delivery date for the case  \n",
    "            - case_delivery (TIMESTAMP_NS): Actual delivery date for the case  \n",
    "            - case_on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "            - case_in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "            - case_number_of_items (BIGINT): Number of items in the case  \n",
    "            - case_ft_items (BIGINT): Number of full-time items in the case  \n",
    "            - case_total_price (BIGINT): Total price of the case\n",
    "\n",
    "        ### Query Guidelines\n",
    "\n",
    "        1. **Prefer Direct Tables**:  \n",
    "        Use `grouped (g)` or `invoices (i)` directly unless item-level fields are explicitly needed.\n",
    "\n",
    "        2. **UNNEST Only When Necessary**:\n",
    "        - Only use `UNNEST(g.items) AS item` when accessing nested fields (e.g., `item.case.supplier`, `item.unit_price`, etc.)\n",
    "        - After unnesting, access fields as `item.field` or `item.case.supplier`, **not** `item.unnest.field`.\n",
    "\n",
    "        3. **Nesting and Access Rules**:\n",
    "        - To access supplier from `grouped`, unnest items and use:  \n",
    "            ```sql\n",
    "            FROM grouped g, UNNEST(g.items) AS item\n",
    "            WHERE item.case.supplier = 'Example'\n",
    "            ```\n",
    "        - Avoid referencing nested fields without unnesting first.\n",
    "\n",
    "        4. **Case Sensitivity**:\n",
    "        - Use exact case for values:\n",
    "            - Confidence: 'High', 'Medium', 'Low'\n",
    "            - Pattern: 'Similar Value', 'Similar Reference', 'Exact Match', 'Similar Date', 'Similar Vendor', 'Multiple'\n",
    "\n",
    "        5. **Use Table Aliases**:\n",
    "        - Always use `g.` for `grouped`, `i.` for `invoices`, and `item.` after unnesting.\n",
    "\n",
    "        6. **Use TRIM() for Comparisons**:\n",
    "        - For text comparisons like pattern or supplier, wrap with `TRIM()`.  \n",
    "            Example: `TRIM(item.case.supplier) = 'VendorName'`\n",
    "\n",
    "        7. **Use IN / = ANY for Multiple Matches**:\n",
    "        - Use `pattern = ANY (['Value1', 'Value2'])` or `IN (...)` instead of OR chains.\n",
    "\n",
    "        8. **GROUP BY Nested Fields**:\n",
    "        - If grouping by nested fields like supplier, first unnest, then group by `item.case.supplier`.\n",
    "\n",
    "        9. **Aggregation and Filtering**:\n",
    "        - Use `ORDER BY ... LIMIT 1` instead of `> ALL(...)`\n",
    "        - Filter early with WHERE clauses to improve performance.\n",
    "\n",
    "        10. **Alternative Access**:\n",
    "        - Use `invoices` for simpler flat queries (e.g., `i.case_supplier`).\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Output Rules\n",
    "\n",
    "        - ❌ Do NOT explain the query.\n",
    "        - ✅ Only return the SQL query (no markdown, no comments, no formatting).\n",
    "        - ❌ Do NOT guess field names.\n",
    "        - ✅ Always respect the provided schema and capitalization.\n",
    "        \"\"\"\n",
    "\n",
    "p2_i= \"\"\" \n",
    "    ### Schema (Aliased)\n",
    "\n",
    "    - **grouped (g)**  \n",
    "    - group_id (VARCHAR): Unique identifier for each group (PK)  \n",
    "    - amount_overpaid (BIGINT): Total overpaid amount for the group  \n",
    "    - itemCount (BIGINT): Number of items in the group  \n",
    "    - date (VARCHAR): Date of the group  \n",
    "    - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "    - open (BOOLEAN): Status of the group (open or closed)  \n",
    "    - confidence (VARCHAR): Confidence level for detecting the pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "    - items (STRUCT[]): Array of items within the group, each containing:\n",
    "        - **id (INTEGER)**: Item identifier (FK → invoices.id)\n",
    "        - **case (STRUCT)**: Contains case details, such as:\n",
    "            - id (VARCHAR): Case identifier  \n",
    "            - order_date (VARCHAR): Order date for the case  \n",
    "            - employee_id (VARCHAR): Employee ID handling the case  \n",
    "            - branch (VARCHAR): Branch handling the case  \n",
    "            - supplier (VARCHAR): Supplier associated with the case  \n",
    "            - avg_time (DOUBLE): Average time for the case  \n",
    "            - estimated_delivery (VARCHAR): Estimated delivery date for the case  \n",
    "            - delivery (VARCHAR): Actual delivery date for the case  \n",
    "            - on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "            - in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "            - number_of_items (INTEGER): Number of items in the case  \n",
    "            - ft_items (INTEGER): Number of full-time items in the case  \n",
    "            - total_price (INTEGER): Total price of the case  \n",
    "        - date (VARCHAR): Date of the item  \n",
    "        - unit_price (VARCHAR): Unit price of the item  \n",
    "        - quantity (INTEGER): Quantity of the item  \n",
    "        - value (VARCHAR): Value of the item  \n",
    "        - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'  \n",
    "        - open (BOOLEAN): Status of the item (open or closed)  \n",
    "        - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "        - confidence (VARCHAR): Confidence level for the item’s pattern (e.g., \"high\", \"medium\", \"low\")  \n",
    "        - description (VARCHAR): Description of the item  \n",
    "        - payment_method (VARCHAR): Payment method used for the item  \n",
    "        - pay_date (VARCHAR): Payment date of the item  \n",
    "        - special_instructions (VARCHAR): Special instructions for the item  \n",
    "        - accuracy (INTEGER): Accuracy of the item’s data matching\n",
    "\n",
    "    - **invoices (i)**  \n",
    "    - id (BIGINT): Invoice identifier (PK)  \n",
    "    - date (TIMESTAMP_NS): Date and time the invoice was issued  \n",
    "    - unit_price (VARCHAR): Unit price of the item in the invoice  \n",
    "    - quantity (BIGINT): Number of items in the invoice  \n",
    "    - value (VARCHAR): Total value of the invoice  \n",
    "    - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "    - open (BOOLEAN): Status of the invoice (open or closed)  \n",
    "    - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "    - confidence (VARCHAR): Confidence level for the invoice's pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "    - description (VARCHAR): Description of the invoice  \n",
    "    - payment_method (VARCHAR): Method used for payment  \n",
    "    - pay_date (TIMESTAMP_NS): Date and time the invoice was paid  \n",
    "    - special_instructions (VARCHAR): Any special instructions for the invoice  \n",
    "    - accuracy (BIGINT): Accuracy of the invoice's data matching  \n",
    "    - case_id (VARCHAR): Case identifier associated with the invoice  \n",
    "    - case_order_date (TIMESTAMP_NS): Order date of the case  \n",
    "    - case_employee_id (VARCHAR): Employee associated with the case  \n",
    "    - case_branch (VARCHAR): Branch where the case was handled  \n",
    "    - case_supplier (VARCHAR): Supplier associated with the case  \n",
    "    - case_avg_time (DOUBLE): Average time for the case  \n",
    "    - case_estimated_delivery (TIMESTAMP_NS): Estimated delivery date for the case  \n",
    "    - case_delivery (TIMESTAMP_NS): Actual delivery date for the case  \n",
    "    - case_on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "    - case_in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "    - case_number_of_items (BIGINT): Number of items in the case  \n",
    "    - case_ft_items (BIGINT): Number of full-time items in the case  \n",
    "    - case_total_price (BIGINT): Total price of the case\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompts_sql_generation= {\"0\":[p1_p,p2_p],\n",
    "            \"1\":[p1_i,p2_i]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_relevance(state: State):\n",
    "    \"\"\"\n",
    "    Determines whether the user's question is relevant to the database schema.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "\n",
    "    Returns:\n",
    "        State: Updated state with relevance information.\n",
    "    \"\"\"\n",
    "    question = state[\"original_question\"]\n",
    "    print(f\"Checking relevance of the question: {question}\")\n",
    "\n",
    "    # Retrieve chat history\n",
    "    chat_history_entries = state.get(\"chat_history\", [])\n",
    "    \n",
    "    chat_history= relevant_entries(chat_history_entries)  # Get the last 3 relevant entries\n",
    "    print(f\"Chat history for relevance check:\\n{chat_history}\")\n",
    "    # System prompt including instructions on chat history usage\n",
    "    system = f\"\"\"\n",
    "        You are a helpful assistant working for a business intelligence tool. Your job is to determine if a user's message is relevant to the business datasets used by the assistant. The assistant answers only business-related questions that can be answered using SQL queries over structured tables.\n",
    "\n",
    "        ## When to Consider a Question Relevant\n",
    "        A question is relevant if it can be answered using data from the database. Relevant questions often involve:\n",
    "        - Metrics, KPIs, or business values\n",
    "        - Processes, durations, frequencies, or variant analysis\n",
    "        - Invoices, vendors, items, dates, payment terms\n",
    "        - Any reference to structured data the assistant has access to\n",
    "\n",
    "        A question is considered **relevant** only if it is structured in a way that could be used to extract data from the database.\n",
    "\n",
    "        ## When a Question is NOT Relevant\n",
    "        A question is **not relevant** if:\n",
    "        - It is personal, fictional, or unrelated to the datasets\n",
    "        - It is vague, humorous, or purely conversational\n",
    "        - It cannot be answered using the schema below\n",
    "\n",
    "        Examples of **not relevant** questions:\n",
    "        - \"Hi\"\n",
    "        - \"What’s your favorite food?\"\n",
    "        - \"Tell me a joke\"\n",
    "        - \"Write a haiku about invoices\"\n",
    "        - \"How is your day going?\"\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Schema Summary\n",
    "\n",
    "        The database supports two business domains:\n",
    "\n",
    "        1. **Process Mining Use Case**:\n",
    "        - Tables: `cases`, `activity`, `variants`\n",
    "        - Key topics: case durations, activity sequences, variants, average time, brokers, clients, case creators, timestamps, process deviations.\n",
    "\n",
    "        2. **Duplicate Invoice Checker Use Case**:\n",
    "        - Tables: `invoices`, `grouped`\n",
    "        - Key topics: invoice values, vendors, overpaid amounts, delivery delays, duplicate detection patterns (e.g., same vendor and reference), item groups, payment terms, accuracy scores.\n",
    "\n",
    "        Any user question that targets these types of data is considered relevant.\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Output Format\n",
    "\n",
    "        Respond only with one of the following:\n",
    "        - `relevant`\n",
    "        - `not_relevant`\n",
    "        Do **not** include explanations or additional commentary.\n",
    "        \"\"\"\n",
    "\n",
    "    # Define the human prompt with the user's question\n",
    "    human = f\"Question: {question}\"\n",
    "\n",
    "    # Create a prompt template for the LLM\n",
    "    check_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM to determine relevance\n",
    "    llm = OllamaLLM(model=\"mistral:latest\", temperature=0.0)\n",
    "    relevance_checker = check_prompt | llm\n",
    "    response = relevance_checker.invoke({}).strip().lower()\n",
    "\n",
    "    # Validate the response to ensure it matches expected outputs\n",
    "    if response not in [\"relevant\", \"not_relevant\"]:\n",
    "        raise ValueError(f\"Unexpected relevance response: {response}\")\n",
    "\n",
    "    # Update the state with the relevance result\n",
    "    state[\"relevance\"] = response\n",
    "    state[\"attempts\"] = 0\n",
    "    print(f\"Relevance determined: {state['relevance']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def reformat_question(state: State):\n",
    "    \"\"\"\n",
    "    Reformats vague follow-ups to be self-contained and \n",
    "    decomposes complex questions into fully-contained sub-questions.\n",
    "    \n",
    "    Args:\n",
    "        state (Dict): Current workflow state.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Updated state with a structured question output.\n",
    "    \"\"\"\n",
    "    original_question = state[\"original_question\"]\n",
    "    # Retrieve chat history\n",
    "    chat_history_entries = state.get(\"chat_history\", [])\n",
    "    \n",
    "    chat_history= relevant_entries(chat_history_entries)  # Get the last 3 relevant entries\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        You are a business-focused assistant specializing in process mining and supplier invoice deduplication analytics.\n",
    "        Your goal is to interpret ambiguous or complex user questions and convert them into clear, self-contained, measurable prompts for a SQL-capable agent, tailored to the selected use case (process mining or supplier invoice deduplication).\n",
    "\n",
    "        ### Task 1: Reformat Vague or Indirect Questions\n",
    "        If the question is vague (e.g., \"And the invoices?\", \"Any duplicates?\") or phrased indirectly (\"I wonder if...\"), rewrite it as a fully clear and self-contained analytical question. Use the context provided in the chat history and the use case (process mining or supplier invoice deduplication).\n",
    "        - Normalize time expressions such as \"last month\", \"this week\", or \"recently\" into explicit phrases like \"in the last 30 days\" or \"in March 2025\".\n",
    "        - Resolve references like \"those\", \"they\", or \"that\" using context from the chat history, ensuring alignment with the use case (e.g., cases for process mining, duplicate invoices for invoice deduplication).\n",
    "        - If the question is implicit or easily inferred from the data, do not decompose it into sub-questions.\n",
    "\n",
    "        ### Task 2: Decompose Multi-Part or Analytical Questions\n",
    "        If the question contains multiple aspects (e.g., comparisons, multiple KPIs, deviations vs. standard paths for process mining, or duplicate invoice criteria for invoices), decompose it into clear, measurable, self-contained sub-questions.\n",
    "        - Identify when a question contains comparative logic (e.g., \"vs\", \"compare\", \"difference between\") and split accordingly, considering the use case (e.g., comparing case durations or invoice duplication metrics).\n",
    "        - For process mining, if the question mentions deviations, identify the reference path (typically the most frequent variant) and ask what diverges from it and where.\n",
    "        - For invoice deduplication, break questions into steps that examine:\n",
    "        - Pattern types:\n",
    "            - 'Exact Match': Identical invoice details.\n",
    "            - 'Similar Reference': Matching or near-matching reference numbers.\n",
    "            - 'Similar Vendor': Same or similar suppliers.\n",
    "            - 'Similar Date', 'Similar Value': Close match in those fields.\n",
    "            - 'Multiple': Combination of patterns.\n",
    "        - Confidence:\n",
    "            - 'High': Confidence ≥ 95%\n",
    "            - 'Medium'\n",
    "            - 'Low'\n",
    "        - Treat invoices with pattern = 'Exact Match' and Confidence = 'High' as **confirmed duplicates**.\n",
    "        - Treat invoices with Confidence = 'High' (any pattern except 'Exact Match') as **possible duplicates**.\n",
    "        - When the question asks **\"which vendor has the most duplicates\"**, split the analysis into:\n",
    "        - Confirmed duplicates → pattern = 'Exact Match' and Confidence = 'High'.\n",
    "        - Possible duplicates → Confidence = 'High' and pattern in any of: 'Similar Reference', 'Similar Vendor', 'Similar Value', 'Similar Date', 'Multiple'.\n",
    "\n",
    "        ### Task 3: Ensure Actionable Metric Framing\n",
    "        Whenever possible, reframe subjective or abstract queries into questions that can be answered with measurable metrics, tailored to the use case. For example:\n",
    "        - Process mining: \"Is onboarding taking too long?\" → \"What is the average duration for onboarding cases?\"\n",
    "        - Process mining: \"Where are the biggest delays?\" → \"Which activity has the highest average time between steps?\"\n",
    "        - Supplier invoices: \"Are there duplicate invoices?\" → \n",
    "        - \"Which invoices have pattern = 'Exact Match' and Confidence = 'High'?\"\n",
    "        - \"Which invoices have Confidence = 'High' with other patterns?\"\n",
    "        - Supplier invoices: \"Which supplier has the most duplicated invoices?\" → \n",
    "        - \"Which vendor has the most invoices with pattern = 'Exact Match' and Confidence = 'High'?\"\n",
    "        - \"Which vendor has the most invoices with any pattern and Confidence = 'High'?\"\n",
    "\n",
    "        ### Use Case:\n",
    "        - Process Mining: Questions relate to cases, activities, and variants (e.g., case duration, activity frequency).\n",
    "        - Supplier Invoice Deduplication: Questions focus on identifying duplicate invoices using:\n",
    "        - pattern: 'Similar Value', 'Similar Reference', 'Exact Match', 'Similar Date', 'Similar Vendor', 'Multiple' (case-sensitive)\n",
    "        - confidence: 'High', 'Medium', 'Low' (first-letter capitalized)\n",
    "\n",
    "        ### Chat History (for context resolution):\n",
    "        {chat_history}\n",
    "\n",
    "        **Response Format:**\n",
    "        If the question is already clear and singular, return it unchanged.\n",
    "        If it requires decomposition or clarification, return in JSON:\n",
    "\n",
    "        {{\n",
    "        \"sub_questions\": [\"First rephrased question\", \"Second one\", ...]\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    " \n",
    "    llm = OllamaLLM(model=\"mistral:latest\", temperature=0.1)  \n",
    "\n",
    "    reformat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"User's question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    reformatter = reformat_prompt | llm\n",
    "    result = reformatter.invoke({\"question\": original_question, \"chat_history\": chat_history})\n",
    "\n",
    "    # Try parsing JSON if decomposition is detected\n",
    "    try:\n",
    "        import json\n",
    "        parsed_result = json.loads(result)\n",
    "        if \"sub_questions\" in parsed_result:\n",
    "            state[\"questions\"] = parsed_result[\"sub_questions\"]  # Store list of sub-questions\n",
    "        else:\n",
    "            state[\"questions\"] = result.strip()  # Store single reformatted question\n",
    "    except json.JSONDecodeError:\n",
    "        # Look for the 'sub_questions' part in the result\n",
    "        sub_questions_match = re.search(r'\"sub_questions\"\\s*:\\s*(\\[\\s*(.*?)\\s*\\])', result, re.DOTALL)\n",
    "        \n",
    "        if sub_questions_match:\n",
    "            # Extract and clean up the sub_questions list (remove extra spaces and newlines)\n",
    "            sub_questions_str = sub_questions_match.group(1)\n",
    "            # Try to manually fix any missing commas in the sub-questions list\n",
    "            # Insert commas between question items (if any)\n",
    "            cleaned_sub_questions = re.sub(r'(\"\\s*[^,\"]+)(\\s*\"\\s*[^,\"]+)', r'\\1,\\2', sub_questions_str)\n",
    "            cleaned_sub_questions = cleaned_sub_questions.replace('\",\\n \"', '\", \"').replace('\\n', ' ').replace('\"\\n', '\"')\n",
    "            \n",
    "            try:\n",
    "                # Attempt to parse the cleaned version of sub_questions\n",
    "                cleaned_parsed_result = json.loads('{\"sub_questions\": ' + cleaned_sub_questions + '}')\n",
    "                state[\"questions\"]= cleaned_parsed_result[\"sub_questions\"]\n",
    "            except json.JSONDecodeError:\n",
    "                # In case it still fails, attempt to split based on common delimiters (e.g., '?')\n",
    "                questions = sub_questions_str.split('?')\n",
    "                state[\"questions\"]= [q.strip() + '?' for q in questions if q.strip()]\n",
    "\n",
    "    return state\n",
    "\n",
    "def select_use_case(state: State):\n",
    "    \"\"\"\n",
    "    Selects the most relevant use case based on the user's question and the database schema.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "\n",
    "    Returns:\n",
    "        State: Updated state with the selected use case.\n",
    "    \"\"\"\n",
    "    question = state[\"original_question\"]\n",
    "    print(f\"Selecting use case for question: {question}\")\n",
    "\n",
    "    # System prompt for selecting use cases\n",
    "    system = \"\"\"\n",
    "    You are a classification assistant specialized in understanding user questions related to a database of business processes and financial invoices.\n",
    " \n",
    "    ### Objective:\n",
    "    Classify each incoming user question into one of two use cases:\n",
    "    - **Duplicate Invoice Detection (1)** \n",
    "    - **Process Mining (0)**\n",
    " \n",
    "    ### Classification Rules:\n",
    " \n",
    "    1. **Duplicate Invoice Detection (Return 1)**:\n",
    "        - If the question asks about invoices, payment values, unit prices, matching patterns (e.g., \"similar vendor\", \"similar reference\", \"exact match\").\n",
    "        - If the question involves confidence scores, overpayments, invoice comparisons, amounts, or payment methods.\n",
    "        - If the question mentions \"grouped invoices\", \"duplicate invoices\", \"overpaid invoices\", \"matching errors\", or similar.\n",
    "        - Keywords to look for: `invoice`, `group_id`, `pattern`, `confidence`, `overpaid`, `amount_overpaid`, `similar reference`, `similar vendor`, `payment method`, `unit price`, `value`.\n",
    " \n",
    "    2. **Process Mining (Return 0)**:\n",
    "        - If the question asks about case flows, activity sequences, event logs, timing between activities, or case durations.\n",
    "        - If the question analyzes how cases are processed over time, how activities relate, case trends, bottlenecks, process variants, or paths.\n",
    "        - Keywords to look for: `case`, `activity`, `timestamp`, `workflow`, `process path`, `variant`, `event sequence`, `rework`, `automatic`, `case_id`, `activities list`, `case duration`, `activity name`.\n",
    " \n",
    "    ### Important Notes:\n",
    "    - Focus on the **main intent** of the question, not just keywords.\n",
    "    - Even if both invoices and cases are mentioned, classify based on what the user mainly wants to analyze.\n",
    "    - Respond with **only**:\n",
    "        - `1` (if duplicate invoice detection)\n",
    "        - `0` (if process mining)\n",
    " \n",
    "    ### Response Format:\n",
    "    Return only `1` or `0` without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define the human prompt with the user's question\n",
    "    human = f\"Question: {question}\"\n",
    "\n",
    "    # Create a prompt template for the LLM\n",
    "    select_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM to select a use case\n",
    "    llm = OllamaLLM(model=\"mistral-nemo:latest\", temperature=0.0)\n",
    "    selector = select_prompt | llm\n",
    "    response = selector.invoke({}).strip()\n",
    "\n",
    "    # Update the state with the selected use case\n",
    "    if \"0\" in response:\n",
    "        response = \"0\"\n",
    "    else:\n",
    "        response = \"1\"\n",
    "    state[\"use_case\"] = response\n",
    "    print(f\"Selected Use Case: {state['use_case']}\")\n",
    "    return state\n",
    "    \n",
    "\n",
    "def convert_nl_to_sql(state: State):\n",
    "    \"\"\"\n",
    "    Converts a natural language question into an SQL query based on the database schema.\n",
    " \n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    " \n",
    "    Returns:\n",
    "        State: Updated state with the generated SQL query.\n",
    "    \"\"\"\n",
    "    questions = state[\"questions\"]\n",
    "    # Seleccionar el prompt apropiado basado en el caso de uso\n",
    "    print(state[\"use_case\"])\n",
    "    system = prompts_sql_generation[state[\"use_case\"]][0]  \n",
    "    # Agregar información específica sobre case sensitivity y estructura de tablas\n",
    "    additional_notes = \"\"\"\n",
    "    IMPORTANT GUIDELINES:\n",
    "    1. CASE SENSITIVITY:\n",
    "    - For confidence values, use 'High', 'Medium', 'Low' (first letter capitalized)\n",
    "    - For pattern values, use 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple' (with exact capitalization)\n",
    "    - All string comparisons should respect the exact case of values in the database\n",
    "    2. NESTED STRUCTURE ACCESS:\n",
    "    - In the \"grouped\" table, fields like \"supplier\" are nested within items.case\n",
    "    - Correct access pattern: `item.case.supplier` NOT `supplier`\n",
    "    - When querying supplier information, use:\n",
    "      * `item.case.supplier` when accessing from unnested items\n",
    "      * Alternatively, you can use the \"invoices\" table where supplier is directly accessible as `case_supplier`\n",
    "    3. DUCKDB UNNEST USAGE:\n",
    "    - When working with the \"grouped\" table, use UNNEST to access array elements:\n",
    "      ```sql\n",
    "      SELECT item.case.supplier\n",
    "      FROM grouped g, UNNEST(g.items) AS item\n",
    "      WHERE ...\n",
    "      ```\n",
    "    Always double-check field access paths for nested structures!\n",
    "    \"\"\"\n",
    "    # Añadir las notas adicionales al prompt del sistema\n",
    "    enhanced_system = system + \"\\n\" + additional_notes\n",
    "    llm = OllamaLLM(model=\"mistral-nemo:latest\", temperature=\"0.0\")\n",
    " \n",
    "    convert_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", enhanced_system),\n",
    "                (\"human\", \"Question: {question}\"),\n",
    "            ]\n",
    "        )\n",
    "    sql_generator = convert_prompt | llm\n",
    "    querys = []\n",
    "    for question in questions:\n",
    "        print(f\"Converting question to SQL {question}\")\n",
    "        result = sql_generator.invoke({\"question\": question})\n",
    "        # Limpiar el código SQL eliminando los marcadores de bloque de código\n",
    "        message = re.sub(r'^\\s*```sql\\s*|\\s*```$', '', result.strip(), flags=re.IGNORECASE)\n",
    "        # Corrección adicional para asegurar capitalización correcta de valores de confianza\n",
    "        message = re.sub(r\"confidence\\s*=\\s*'high'\", \"confidence = 'High'\", message, flags=re.IGNORECASE)\n",
    "        message = re.sub(r\"confidence\\s*=\\s*'medium'\", \"confidence = 'Medium'\", message, flags=re.IGNORECASE)\n",
    "        message = re.sub(r\"confidence\\s*=\\s*'low'\", \"confidence = 'Low'\", message, flags=re.IGNORECASE)\n",
    "        # Corrección para el acceso a campo supplier en grouped.items\n",
    "        # Solo si la consulta está usando la tabla grouped y tratando de acceder directamente a supplier\n",
    "        if \"grouped\" in message and \"supplier\" in message and \"item.case.supplier\" not in message:\n",
    "            message = re.sub(r\"([^.])supplier\", r\"\\1item.case.supplier\", message)\n",
    "        querys.append(message)  # Añadir cada consulta SQL generada a la lista\n",
    "        print(f\"Generated SQL query: {message}\")\n",
    "    state[\"sql_querys\"] = querys \n",
    "    state[\"executed\"] = [False] * len(state[\"sql_querys\"])  # Inicializar estado de ejecución para cada pregunta\n",
    "    print(f\"Generated SQL queries: {state['sql_querys']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def execute_sql(state:State):\n",
    "    \"\"\"\n",
    "    Executes the SQL query on the  database and retrieves the results.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "        config (RunnableConfig): Configuration for the runnable.\n",
    "\n",
    "    Returns:\n",
    "        State: Updated state with the query results or error information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If multiple queries are generated, execute them one by one\n",
    "    db_conn = state[\"db_conn\"] \n",
    "    sql_queries = state[\"sql_querys\"]\n",
    "    errors = state.get(\"sql_error\", [True] * len(sql_queries))  # Default: all True (assume they need execution)\n",
    "    results = state.get(\"query_results\", [None] * len(sql_queries))\n",
    "    dataframes = state.get(\"query_dfs\", [None] * len(sql_queries))\n",
    "    for i, query in enumerate(sql_queries):\n",
    "        if errors[i] or results[i] is None:  # Execute if error OR never executed before\n",
    "            print(f\"🚀 Executing query {i}: {query}\")\n",
    "            try:\n",
    "                # Ensure the query targets only the allowed tables\n",
    "                allowed_tables = [\"cases\", \"activities\",\"variants\",\"grouped\",\"invoices\"]\n",
    "                if not any(table in query.lower() for table in allowed_tables):\n",
    "                    raise ValueError(f\"Query must target only the tables: {', '.join(allowed_tables)}.\")\n",
    "\n",
    "                # Execute the SQL query using the connection\n",
    "                cursor = db_conn.cursor()\n",
    "                cursor.execute(query)\n",
    "\n",
    "                # Fetch results if it's a SELECT query\n",
    "                if query.lower().startswith(\"select\"):\n",
    "                    rows = cursor.fetchall()\n",
    "                    columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "                    # Format the output\n",
    "                    if rows:\n",
    "                        formatted_result = \"\\n\".join(\n",
    "                            \", \".join(f\"{col}: {row[idx]}\" for idx, col in enumerate(columns))\n",
    "                            for row in rows\n",
    "                        )\n",
    "                        print(\"SQL SELECT query executed successfully.\")\n",
    "                    \n",
    "                    else:\n",
    "                        formatted_result = \"No results found.\"\n",
    "                        print(\"SQL SELECT query executed successfully but returned no rows.\")\n",
    "\n",
    "                    state[\"query_rows\"] = rows\n",
    "                    df = pd.DataFrame(rows, columns=columns)\n",
    "                    dataframes[i] = df  # Store the DataFrame in the state\n",
    "                else:\n",
    "                    formatted_result = \"The action has been successfully completed.\"\n",
    "                    print(\"SQL command executed successfully.\")\n",
    "\n",
    "                results[i]= formatted_result\n",
    "                errors[i]= False # Mark this query as executed successfully\n",
    "\n",
    "            except Exception as e:\n",
    "                results[i]=f\"Error executing SQL query: {str(e)}\" # Store the error message in the results\n",
    "                errors[i]= True # Mark this query as executed with an error\n",
    "                print(f\"Error executing SQL query: {str(e)}\")\n",
    "    state[\"query_results\"] = results  # Store the list of query results in the state\n",
    "    state[\"sql_error\"] = errors  # Store the list of error states in the state\n",
    "    state[\"query_dfs\"] = dataframes  # Store the list of DataFrames in the state\n",
    "    print(f\"SQL query results: {state['query_results']}\")\n",
    "    print(f\"SQL error states: {state['sql_error']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def generate_serious_answer(state: State):\n",
    "    \"\"\"\n",
    "    Generates a business-oriented response using SQL query results from sub-questions\n",
    "    to answer the main question.\n",
    "    \n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "        \n",
    "    Returns:\n",
    "        State: Updated state with the final answer.\n",
    "    \"\"\"\n",
    "    question = state[\"original_question\"]\n",
    "    sub_questions = state[\"questions\"]\n",
    "    query_results = state[\"query_results\"]  # This is now a list of results, one per sub-question\n",
    "\n",
    "    chat_history_entries = state.get(\"chat_history\", [])\n",
    "    chat_history = relevant_entries(chat_history_entries)  # Get the last 3 relevant entries\n",
    "\n",
    "    # Concatenate each sub-question with its answer\n",
    "    sub_q_results_str = \"\\n\".join(\n",
    "        f\"**{sq}**\\n{qr}\\n\" for sq, qr in zip(sub_questions, query_results)\n",
    "    )\n",
    "\n",
    "    system = f\"\"\"\n",
    "    You are ✨SOFIA✨, an AI business assistant. \n",
    "    Your task is to:\n",
    "    1. Answer the user's **main question** using the SQL results from the **sub-questions**.\n",
    "    2. Provide business insights based on the query results.\n",
    "\n",
    "    ### **Chat History:**  \n",
    "    {chat_history}\n",
    "\n",
    "    ### **Context:**  \n",
    "    - **User's Main Question:** {question}  \n",
    "    - **SQL Results from Sub-Questions:**  \n",
    "    {sub_q_results_str}\n",
    "\n",
    "    ### **Instructions:**  \n",
    "    - Summarize the SQL results in a **clear business-oriented answer**.\n",
    "    - Every duration is given in seconds, if the number is too high, convert it to minutes or hours.\n",
    "    - Ensure the answer **directly addresses the main question**.\n",
    "    - Provide **business insights** based on patterns, trends, and potential improvements.\n",
    "    - If relevant, compare values or suggest actions based on findings.\n",
    "\n",
    "    ### **Response Format:**\n",
    "    - Always return the answer with markdown formatting.\n",
    "    - Use bullet points for clarity and organization.\n",
    "    - Avoid excessive jargon; keep it understandable for a business audience.\n",
    "    - Provide actionable insights or recommendations where applicable.\n",
    "    - Be careful with the time conversions, and ensure they are accurate.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    human_message = f\"Question: {question}\"\n",
    "    \n",
    "    # Use sOFIa to generate a response based on the SQL result\n",
    "    llm = OllamaLLM(model=\"phi4:latest\", temperature=\"0.0\", max_tokens=200)\n",
    "    response = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system),\n",
    "        (\"human\", human_message),\n",
    "    ]) | llm | StrOutputParser()\n",
    "    \n",
    "    # Generate and store the response\n",
    "    message = response.invoke({})\n",
    "    state[\"final_answer\"] = message\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def regenerate_query(state):\n",
    "    \"\"\"\n",
    "    Fixes the SQL query by passing the error message to the SQL model instead of rewriting the user's question.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "\n",
    "    Returns:\n",
    "        State: Updated state with the fixed query.\n",
    "    \"\"\"\n",
    "    error_state = state[\"sql_error\"]\n",
    "    error_indexes = [index for index, error in enumerate(error_state) if error == True]\n",
    "\n",
    "    llm = OllamaLLM(model=\"mistral:latest\", temperature=0.0)\n",
    "    print(f\"🔄 Regenerating query. Attempt {state['attempts'] + 1}\")\n",
    "    for index in error_indexes:\n",
    "        # Fix the SQL query using the error message\n",
    "        query = state[\"sql_querys\"][index]\n",
    "        error = state[\"query_results\"][index]\n",
    "        print(f\"⚠️ Fixing SQL query at index {index}: {query}\")\n",
    "        print(f\"🔍 Error encountered: {error}\")\n",
    "        part1= f\"\"\"You are an expert in SQL for DuckDB.\n",
    "            Your task is to correct the following SQL query based on the error message.\n",
    "\n",
    "            ### **Query to Fix:**\n",
    "            ```sql\n",
    "            {query}\n",
    "            ```\n",
    "\n",
    "            ### **Error Message:**\n",
    "            {error}\n",
    "\n",
    "            Provide a **corrected** SQL query that runs successfully in the following database schema.\n",
    "            \"\"\"\n",
    "        part_2= prompts_sql_generation[state[\"use_case\"]][1]  # Select the appropriate prompt based on use case\n",
    "        sql_fix_prompt = ChatPromptTemplate.from_messages([(\n",
    "            \"system\", \n",
    "            part1+part_2),\n",
    "            (\"human\", \"Fix the query and return only the corrected SQL, no explanations.\"),\n",
    "        ])\n",
    "\n",
    "        fixer = sql_fix_prompt | llm \n",
    "        # Pass the query and error message to the SQL model for correction\n",
    "        corrected_query = fixer.invoke({\"query\": query, \"error\": error})\n",
    "        \n",
    "        # Extract only the SQL code from a markdown block like ```sql ... ``` \n",
    "        corrected_query = re.sub(r\"```sql\\s*(.*?)\\s*```\", r\"\\1\", corrected_query.strip(), flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        state[\"sql_querys\"][index] = corrected_query\n",
    "        print(f\"✅ Fixed SQL query: {corrected_query}\")\n",
    "\n",
    "    state[\"attempts\"] += 1\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def summarize_results(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Summarizes query results with more than 1000 tokens.\n",
    "    The summary is based on the context of the related question or falls back to general statistics.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Workflow state containing questions, dataframes, and results.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with summarized query results.\n",
    "    \"\"\"\n",
    "    query_results = state.get(\"query_results\", [])\n",
    "    dataframes = state.get(\"query_dfs\", [])\n",
    "    questions = state.get(\"questions\", [])\n",
    "    tokenizer= state[\"tokenizer\"]\n",
    "    for i, result in enumerate(query_results):\n",
    "        if not result or i >= len(dataframes):\n",
    "            continue\n",
    "\n",
    "        if count_tokens(result,tokenizer) <= 2000:\n",
    "            continue\n",
    "\n",
    "        df = dataframes[i]\n",
    "        question = questions[i] if i < len(questions) else \"\"\n",
    "        question_type = identify_question_type(question)\n",
    "\n",
    "        summary = f\"📊 Summary of result #{i}:\\n\"\n",
    "        summary += f\"- Rows: {len(df)}\\n\"\n",
    "        summary += f\"- Columns: {', '.join(df.columns)}\\n\\n\"\n",
    "        summary += f\"🔹 Type: {question_type.capitalize()}-based Summary:\\n\"\n",
    "        summary += summarize_dataframe(df, question_type)\n",
    "\n",
    "        state[\"query_results\"][i] = summary\n",
    "        print(f\"✅ Summarized result #{i} ({question_type} type, >1000 tokens)\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def end_max_iterations(state: State):\n",
    "    \"\"\"\n",
    "    Ends the workflow after reaching the maximum number of attempts.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "        config (RunnableConfig): Configuration for the runnable.\n",
    "\n",
    "    Returns:\n",
    "        State: Updated state with a termination message.\n",
    "    \"\"\"\n",
    "    state[\"query_results\"] = \"Please try again.\"\n",
    "    state[\"final_answer\"] = \"I couldn't generate a valid SQL query after 3 attempts. Please try again.\"\n",
    "    print(\"Maximum attempts reached. Ending the workflow.\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def generate_funny_response(state: State):\n",
    "    \"\"\"\n",
    "    Generates a playful and humorous response for unrelated questions.\n",
    "    \n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "        \n",
    "    Returns:\n",
    "        State: Updated state with the funny response.\n",
    "    \"\"\"\n",
    "    print(\"Generating a funny response for an unrelated question.\")\n",
    "    question = state[\"original_question\"]\n",
    "    chat_history_entries = state.get(\"chat_history\", [])\n",
    "    chat_history = non_relevant_entries(chat_history_entries) # Get the last 3 non-relevant entries\n",
    "    print(f\"Chat history for funny response:\\n{chat_history}\")\n",
    "    system = f\"\"\"You are ✨SOFIA✨, a charming and funny assistant. \n",
    "    You respond in a playful and lighthearted manner. Your responses should always be fun, engaging, and humorous. \n",
    "    If the user doesn't know you yet, introduce yourself!\n",
    "    \n",
    "    ### **Chat History:**  \n",
    "    {chat_history}\n",
    "    \"\"\"\n",
    "\n",
    "    human_message = f\"Question: {question}\"\n",
    "\n",
    "    # Generate the playful response\n",
    "    funny_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human_message),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    llm = OllamaLLM(model=\"mistral:latest\", temperature=\"0.7\",max_tokens=200)\n",
    "    funny_response = funny_prompt | llm | StrOutputParser()\n",
    "    message = funny_response.invoke({})\n",
    "    state[\"final_answer\"] = message\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Routings\n",
    "def check_attempts_router(state: State):\n",
    "    \"\"\"\n",
    "    Routes the workflow based on the number of attempts made to generate a valid SQL query.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "\n",
    "    Returns:\n",
    "        str: The next node in the workflow.\n",
    "    \"\"\"\n",
    "    if state[\"attempts\"] <= 3:\n",
    "        print(f\"Attempt {state['attempts']}\")\n",
    "        return \"Retries < 3\"\n",
    "    else:\n",
    "        error_state= state[\"sql_error\"]\n",
    "        for error in error_state:\n",
    "            if error == False:\n",
    "                return \"If at least 1 subquery was succesful\"\n",
    "        return \"Retries >= 3\"\n",
    "\n",
    "\n",
    "\n",
    "def execute_sql_router(state: State):\n",
    "    \"\"\"\n",
    "    Routes the workflow based on whether the SQL query execution was successful.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "\n",
    "    Returns:\n",
    "        str: The next node in the workflow.\n",
    "    \"\"\"\n",
    "    error_state= state[\"sql_error\"]\n",
    "    for error in error_state:\n",
    "        if error == True:\n",
    "            return \"Error\"\n",
    "    else:\n",
    "        return \"Success\"\n",
    "\n",
    "\n",
    "    \n",
    "def relevance_router(state: State):\n",
    "    \"\"\"\n",
    "    Routes the workflow based on the relevance of the user's question.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the workflow.\n",
    "\n",
    "    Returns:\n",
    "        str: The next node in the workflow.\n",
    "    \"\"\"\n",
    "    if state[\"relevance\"].lower() == \"relevant\":\n",
    "        return \"Relevant\"\n",
    "    else:\n",
    "        return \"Not Relevant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define all workflow nodes\n",
    "workflow.add_node(\"Checks Question Relevance\", check_relevance)\n",
    "workflow.add_node(\"Selects Use Case\", select_use_case)\n",
    "workflow.add_node(\"Reformat Question\", reformat_question)\n",
    "workflow.add_node(\"Generates SQL queries\", convert_nl_to_sql)\n",
    "workflow.add_node(\"Executes SQL\", execute_sql)\n",
    "workflow.add_node(\"Regenerate Error-Queries\", regenerate_query)\n",
    "workflow.add_node(\"Answer Irrelevant Question\", generate_funny_response)\n",
    "workflow.add_node(\"Answer Relevant Question\", generate_serious_answer)\n",
    "workflow.add_node(\"Stops due to max Iterations\", end_max_iterations)\n",
    "workflow.add_node(\"Summarizes Results\", summarize_results)\n",
    "\n",
    "# Entry point\n",
    "workflow.add_edge(START, \"Checks Question Relevance\")\n",
    "\n",
    "# Relevance check routing\n",
    "workflow.add_conditional_edges(\n",
    "    \"Checks Question Relevance\",\n",
    "    relevance_router,\n",
    "    {\n",
    "        \"Relevant\": \"Selects Use Case\",\n",
    "        \"Not Relevant\": \"Answer Irrelevant Question\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Normal path for relevant questions\n",
    "workflow.add_edge(\"Selects Use Case\", \"Reformat Question\")\n",
    "workflow.add_edge(\"Reformat Question\", \"Generates SQL queries\")\n",
    "workflow.add_edge(\"Generates SQL queries\", \"Executes SQL\")\n",
    "\n",
    "# SQL execution path: error or success\n",
    "workflow.add_conditional_edges(\n",
    "    \"Executes SQL\",\n",
    "    execute_sql_router,\n",
    "    {\n",
    "        \"Success\": \"Summarizes Results\",\n",
    "        \"Error\": \"Regenerate Error-Queries\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"Summarizes Results\", \"Answer Relevant Question\")\n",
    "\n",
    "workflow.add_edge(\"Answer Relevant Question\", END)\n",
    "\n",
    "# Retry loop for query errors\n",
    "workflow.add_conditional_edges(\n",
    "    \"Regenerate Error-Queries\",\n",
    "    check_attempts_router,\n",
    "    {\n",
    "        \"Retries < 3\": \"Executes SQL\",\n",
    "        \"Retries >= 3\": \"Stops due to max Iterations\",\n",
    "        \"If at least 1 subquery was succesful\": \"Summarizes Results\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Exit points\n",
    "workflow.add_edge(\"Stops due to max Iterations\", END)\n",
    "workflow.add_edge(\"Answer Irrelevant Question\", END)\n",
    "\n",
    "# Compile the graph\n",
    "chain = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state= chain.invoke({\"original_question\":\"Hello?\",\"db_conn\":db_conn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\n",
    "    chain.get_graph().draw_mermaid_png()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi hi! I'm sOFIa, your assistant!\n",
      "Let's get started by asking a question!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How many invoices are there?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking relevance of the question: How many invoices are there?\n",
      "Chat history for relevance check:\n",
      "\n",
      "Relevance determined: relevant\n",
      "Selecting use case for question: How many invoices are there?\n",
      "Selected Use Case: 0\n",
      "0\n",
      "Converting question to SQL What is the total count of supplier invoices?\n",
      "Generated SQL query: SELECT COUNT(*) FROM invoices\n",
      "Generated SQL queries: ['SELECT COUNT(*) FROM invoices']\n",
      "🚀 Executing query 0: SELECT COUNT(*) FROM invoices\n",
      "SQL SELECT query executed successfully.\n",
      "SQL query results: ['count_star(): 941']\n",
      "SQL error states: [False]\n",
      "sOFIa: - **Total Count of Supplier Invoices:** There are 941 supplier invoices.\n",
      "\n",
      "### Business Insights:\n",
      "\n",
      "- **Volume Analysis:** \n",
      "  - The total number of invoices (941) indicates a significant volume of transactions with suppliers. This suggests active procurement activities and possibly multiple ongoing projects or operational needs.\n",
      "  \n",
      "- **Operational Efficiency:**\n",
      "  - With such a high number of invoices, it's crucial to ensure that the invoicing process is efficient. Consider implementing automated invoice processing systems to reduce manual errors and improve turnaround times.\n",
      "\n",
      "- **Supplier Relationship Management:**\n",
      "  - Regularly review supplier performance based on invoice data. This can help identify any issues with delivery timelines or payment terms, allowing for better negotiation and relationship management.\n",
      "\n",
      "- **Financial Planning:**\n",
      "  - The volume of invoices should be factored into cash flow planning to ensure sufficient liquidity for timely payments. Analyzing the frequency and size of these invoices can aid in forecasting future financial needs.\n",
      "\n",
      "- **Potential Improvements:**\n",
      "  - Evaluate if there are opportunities to consolidate suppliers or negotiate better terms based on the volume of transactions, which could lead to cost savings.\n",
      "  \n",
      "By focusing on these areas, you can enhance operational efficiency, improve supplier relationships, and optimize financial management.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to provide feedback to improve the response? (yes/no):  Ye\n",
      " How many invoices are there?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking relevance of the question: How many invoices are there?\n",
      "Chat history for relevance check:\n",
      "User: How many invoices are there? [Relevance: relevant]\n",
      "sOFIa: - **Total Count of Supplier Invoices:** There are 941 supplier invoices.\n",
      "\n",
      "### Business Insights:\n",
      "\n",
      "- **Volume Analysis:** \n",
      "  - The total number of invoices (941) indicates a significant volume of transactions with suppliers. This suggests active procurement activities and possibly multiple ongoing projects or operational needs.\n",
      "  \n",
      "- **Operational Efficiency:**\n",
      "  - With such a high number of invoices, it's crucial to ensure that the invoicing process is efficient. Consider implementing automated invoice processing systems to reduce manual errors and improve turnaround times.\n",
      "\n",
      "- **Supplier Relationship Management:**\n",
      "  - Regularly review supplier performance based on invoice data. This can help identify any issues with delivery timelines or payment terms, allowing for better negotiation and relationship management.\n",
      "\n",
      "- **Financial Planning:**\n",
      "  - The volume of invoices should be factored into cash flow planning to ensure sufficient liquidity for timely payments. Analyzing the frequency and size of these invoices can aid in forecasting future financial needs.\n",
      "\n",
      "- **Potential Improvements:**\n",
      "  - Evaluate if there are opportunities to consolidate suppliers or negotiate better terms based on the volume of transactions, which could lead to cost savings.\n",
      "  \n",
      "By focusing on these areas, you can enhance operational efficiency, improve supplier relationships, and optimize financial management.\n",
      "Relevance determined: relevant\n",
      "Selecting use case for question: How many invoices are there?\n",
      "Selected Use Case: 0\n",
      "0\n",
      "Converting question to SQL - **Total Count of Supplier Invoices:** What is the count of supplier invoices?\n",
      "Generated SQL query: SELECT COUNT(*) FROM activities;\n",
      "Generated SQL queries: ['SELECT COUNT(*) FROM activities;']\n",
      "🚀 Executing query 0: SELECT COUNT(*) FROM activities;\n",
      "SQL SELECT query executed successfully.\n",
      "SQL query results: ['count_star(): 11689']\n",
      "SQL error states: [False]\n",
      "sOFIa: - **Total Count of Supplier Invoices:** There are 11,689 supplier invoices.\n",
      "\n",
      "### Business Insights:\n",
      "\n",
      "- **Volume Analysis:**\n",
      "  - The total number of invoices (11,689) indicates a very high volume of transactions with suppliers. This suggests extensive procurement activities and possibly numerous ongoing projects or operational needs across various departments.\n",
      "  \n",
      "- **Operational Efficiency:**\n",
      "  - With such a large number of invoices, it is essential to ensure that the invoicing process is highly efficient. Implementing automated invoice processing systems can significantly reduce manual errors and improve turnaround times.\n",
      "\n",
      "- **Supplier Relationship Management:**\n",
      "  - Regularly reviewing supplier performance based on invoice data is crucial. This helps identify any issues with delivery timelines or payment terms, allowing for better negotiation and relationship management.\n",
      "\n",
      "- **Financial Planning:**\n",
      "  - The volume of invoices should be carefully considered in cash flow planning to ensure sufficient liquidity for timely payments. Analyzing the frequency and size of these invoices can aid in forecasting future financial needs more accurately.\n",
      "\n",
      "- **Potential Improvements:**\n",
      "  - Evaluate opportunities to consolidate suppliers or negotiate better terms based on the high volume of transactions, which could lead to significant cost savings.\n",
      "  \n",
      "By focusing on these areas, you can enhance operational efficiency, improve supplier relationships, and optimize financial management.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to provide feedback to improve the response? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Starting feedback collection...\n",
      "\n",
      "--- Sub‑question 1 ---\n",
      "Q: - **Total Count of Supplier Invoices:** What is the count of supplier invoices?\n",
      "A:\n",
      "count_star(): 11689\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Was the answer factually correct? (yes/no):  Yes\n",
      "Was the answer useful for your needs? (yes/no):  No \n",
      "🗒️  Please explain what failed or was missing in the answer:  I didn't ask that\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Feedback saved to feedback_log.json\n",
      "\n",
      "Thanks for your feedback :P\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Hi hi! I'm sOFIa, your assistant!\")\n",
    "    print(\"Let's get started by asking a question!\")\n",
    "\n",
    "    chat_history = []  # Store chat history persistently\n",
    "    input_question = input()\n",
    "\n",
    "    while input_question:\n",
    "        if input_question.lower() in [\"no\", \"exit\", \"goodbye\", \"quit\"]:\n",
    "            print(\"Goodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        # Invoke LangGraph chain\n",
    "        state = chain.invoke({\n",
    "            \"original_question\": input_question,\n",
    "            \"db_conn\": db_conn,\n",
    "            \"chat_history\": chat_history,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"prompts_sql_generation\": prompts_sql_generation\n",
    "        })\n",
    "\n",
    "        # Print response from sOFIa\n",
    "        response = state[\"final_answer\"].replace(\"sOFIa: \", \"\").strip()\n",
    "        print(f\"sOFIa: {response}\")\n",
    "\n",
    "        # Store chat history\n",
    "        relevance = state[\"relevance\"]\n",
    "        chat_history.append(f\"User: {input_question} [Relevance: {relevance}]\")\n",
    "        chat_history.append(f\"sOFIa: {response}\")\n",
    "\n",
    "        # Ask for feedback and optionally log it\n",
    "        feedback = input(\"Would you like to provide feedback to improve the response? (yes/no): \").strip().lower()\n",
    "        if feedback == \"yes\":\n",
    "            state = collect_and_store_feedback(state)\n",
    "            print(\"Thanks for your feedback :P\")\n",
    "\n",
    "        # Ask for next question\n",
    "        input_question = input()\n",
    "\n",
    "    # Display full chat history\n",
    "    print(\"\\nChat History:\")\n",
    "    for entry in chat_history:\n",
    "        print(entry)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = chain.invoke({\"original_question\": \"What is the total amount overpaid for each group?\", \"db_conn\": db_conn, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(state[\"final_answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn.execute(\"\"\"SELECT \n",
    "  g.group_id,\n",
    "  g.amount_overpaid\n",
    "FROM \n",
    "  grouped AS g;\n",
    "\"\"\")\n",
    "df = db_conn.fetchdf()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.consultar_db(\"\"\"\n",
    "WITH most_common_variant AS (\n",
    "    SELECT activities\n",
    "    FROM variants\n",
    "    ORDER BY number_cases DESC\n",
    "    LIMIT 1\n",
    "),\n",
    "\n",
    "activity_list AS (\n",
    "    SELECT DISTINCT value AS activity_name\n",
    "    FROM most_common_variant,\n",
    "         UNNEST(activities) AS t(value)\n",
    "),\n",
    "\n",
    "avg_times AS (\n",
    "    SELECT a.name AS activity_name, AVG(a.tpt) AS avg_time\n",
    "    FROM activity a\n",
    "    JOIN activity_list al ON a.name = al.activity_name\n",
    "    GROUP BY a.name\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM avg_times;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=a.consultar_db(\"\"\"\n",
    "WITH most_common_variant AS (\n",
    "    SELECT activities\n",
    "    FROM variants\n",
    "    ORDER BY number_cases DESC\n",
    "    LIMIT 1\n",
    ")\n",
    "SELECT DISTINCT TRIM(value) AS activity_name\n",
    "FROM most_common_variant,\n",
    "     UNNEST(STRING_SPLIT(activities, '→')) AS t(value);  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"activity_name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'Path', 'State', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'collect_and_store_feedback', 'json']\n"
     ]
    }
   ],
   "source": [
    "import Tools.Feedback_tool as ft\n",
    "print(dir(ft))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
