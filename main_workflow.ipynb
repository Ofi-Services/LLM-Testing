{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2897a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import logging\n",
    "from Tools.Logger import setup_logger\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from db_create import CargaDeArchivos\n",
    "\n",
    "#tools\n",
    "from Tools.Tool import run_sql_workflow, run_think_task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a95c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Logger instantiation ===\n",
    "setup_logger()\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc31642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exitoso\n"
     ]
    }
   ],
   "source": [
    "# === Tokenizer logging ==\n",
    "try:\n",
    "    login(token=\"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\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during tokenizer setup: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# === Database population and connection ===\n",
    "try:\n",
    "    db_manager = CargaDeArchivos()\n",
    "    db_manager.run()\n",
    "    db_conn = db_manager.conn\n",
    "    print('Exitoso')\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during database population and connection: {e}\", exc_info=True)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8c08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Orchestrator state ==\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    plan: List[dict]\n",
    "    current_step: int\n",
    "    results: Dict[str, Any]\n",
    "    query_results: List[str]\n",
    "    db_conn: None\n",
    "    tokenizer: any\n",
    "    use_case: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df70ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == SQL prompts per case of use==\n",
    "p1_p = \"\"\" /no_think\n",
    "    You are an SQL assistant specialized in DuckDB. Your task is to generate accurate SQL queries based on natural language questions, following the schema and rules below.\n",
    "    \n",
    "    ### Schema (Aliased)\n",
    "    \n",
    "    - **cases**  (one row per process instance)\n",
    "        - id (VARCHAR): Unique identifier for each case\n",
    "        - order_date (TIMESTAMP_NS): Date when the order was placed\n",
    "        - employee_id (VARCHAR): ID of the employee handling the case\n",
    "        - branch (VARCHAR): Branch where the order originated\n",
    "        - supplier (VARCHAR): Supplier involved in the case\n",
    "        - avg_time (DOUBLE): Total duration of the case in time units\n",
    "        - estimated_delivery (TIMESTAMP_NS): Expected delivery date\n",
    "        - delivery (TIMESTAMP_NS): Actual delivery date\n",
    "        - on_time (BOOLEAN): Whether the delivery met the deadline\n",
    "        - in_full (BOOLEAN): Whether the order was delivered in full\n",
    "        - number_of_items (INTEGER): Total items in the case\n",
    "        - ft_items (INTEGER): Number of full/complete items delivered\n",
    "        - total_price (DOUBLE): Total price of the order\n",
    "        - total_activities (INTEGER): Number of activities in the case\n",
    "        - rework_activities (INTEGER): Count of repeated/rework activities\n",
    "        - automatic_activities (INTEGER): Count of system-generated activities\n",
    "    \n",
    "    - **activities**  (one row per activity within a process)\n",
    "        - id (INTEGER): Unique identifier for the activity\n",
    "        - timestamp (TIMESTAMP): When the activity occurred\n",
    "        - name (VARCHAR): Name of the activity\n",
    "        - tpt (DOUBLE): Time passed since the previous activity\n",
    "        - user (VARCHAR): Person who performed the activity\n",
    "        - user_type (VARCHAR): Role of the user (e.g., employee, system)\n",
    "        - automatic (BOOLEAN): Whether the activity was system-generated\n",
    "        - rework (BOOLEAN): Whether the activity was a rework/repeat\n",
    "        - case_index (INTEGER): Position of the activity within the case\n",
    "        - case_id (VARCHAR): ID of the associated case\n",
    "        - case_order_date (TIMESTAMP): Order date for the case\n",
    "        - case_employee_id (VARCHAR): Employee ID for the case\n",
    "        - case_branch (VARCHAR): Branch handling the case\n",
    "        - case_supplier (VARCHAR): Supplier involved in the case\n",
    "        - case_avg_time (DOUBLE): Total duration of the case\n",
    "        - case_estimated_delivery (TIMESTAMP): Expected delivery date\n",
    "        - case_delivery (TIMESTAMP): Actual delivery date\n",
    "        - case_on_time (BOOLEAN): Whether the case was delivered on time\n",
    "        - case_in_full (BOOLEAN): Whether the order was complete\n",
    "        - case_number_of_items (INTEGER): Total items in the case\n",
    "        - case_ft_items (INTEGER): Number of full/complete items\n",
    "        - case_total_price (DOUBLE): Total price of the case\n",
    "    \n",
    "    - **variants**  \n",
    "      - id (BIGINT): Variant ID (PK for path)  \n",
    "      - activities (VARCHAR[]): Ordered activity names for this path  \n",
    "      - cases (VARCHAR[]): IDs of cases that followed this path (→ cases.id)  \n",
    "      - number_cases (BIGINT): Total cases following this variant  \n",
    "      - percentage (DOUBLE): Percentage of total cases  \n",
    "      - avg_time (DOUBLE): Avg duration (sec) across cases in this variant\n",
    "    \n",
    "    ### Query Guidelines\n",
    "    \n",
    "    1. Always reference columns with aliases (e.g., c.id, a.case_id).\n",
    "    2. Use UNNEST() in the FROM clause to access list fields like v.activities or v.cases. Do not use UNNEST() inside expressions like = ANY(...).\n",
    "    3. When comparing list values (e.g., activity names), first UNNEST the list in a subquery or CTE, then use direct comparison with TRIM(...).\n",
    "    4. Use TRIM() when comparing activity names (e.g., TRIM(a.name) = TRIM(...)).\n",
    "    5. Avoid unnecessary joins or full scans when possible.\n",
    "    6. Convert time differences with EXTRACT(EPOCH FROM ...).\n",
    "    7. Include all non-aggregated columns in GROUP BY.\n",
    "    \n",
    "    ### Variant Comparison Rules\n",
    "    \n",
    "    - **Most Frequent Path:**  \n",
    "      SELECT * FROM variants WHERE number_cases = (SELECT MAX(number_cases) FROM variants)\n",
    "    \n",
    "    - **Variant Durations:**  \n",
    "      Use avg_time from variants. Do not recompute durations from activities unless explicitly requested.\n",
    "    \n",
    "    - **Deviations:**  \n",
    "      Variants with id different from the most frequent one are deviations.  \n",
    "      To detect deviation points, compare activities with the most frequent variant.\n",
    "    \n",
    "    - **Activity Durations Along Most Frequent Path:**  \n",
    "      1. Extract activities using UNNEST(v.activities) AS activity.  \n",
    "      2. Join with activities table using TRIM(v_activity) = TRIM(a.name).  \n",
    "      3. Group by activity name and compute average tpt.\n",
    "    \n",
    "    ### Common Pitfall Corrections\n",
    "    \n",
    "    - Never use UNNEST() inside = ANY(...). Use UNNEST in a FROM clause or CTE, then join or filter.\n",
    "    - Avoid > ALL(...). Prefer ORDER BY ... LIMIT 1 or = (SELECT MAX(...)).\n",
    "    - Use subqueries for filtered aggregations, like:\n",
    "    \n",
    "      SELECT branch  \n",
    "      FROM cases  \n",
    "      WHERE approved = TRUE  \n",
    "      GROUP BY branch  \n",
    "      ORDER BY AVG(value) DESC  \n",
    "      LIMIT 1\n",
    "    \n",
    "    - When aggregating on top branches, use subqueries or IN with preselected sets.\n",
    "    - If no data matches a filter, return NULL instead of error.\n",
    "    - To detect repeated activities on the same day:\n",
    "    \n",
    "      SELECT a.case_id, DATE_TRUNC('day', a.timestamp), COUNT(*)  \n",
    "      FROM activities AS a  \n",
    "      GROUP BY a.case_id, DATE_TRUNC('day', a.timestamp)  \n",
    "      HAVING COUNT(*) > 1\n",
    "    \n",
    "      (Avoid GENERATE_SERIES here.)\n",
    "    \n",
    "    ### Error Examples\n",
    "    \n",
    "    *Incorrect:*\n",
    "    \n",
    "    ```sql\n",
    "    SELECT branch FROM activities;\n",
    "    -- Error: 'branch' does not exist in 'activities'\n",
    "\n",
    "    SELECT case.id, name FROM grouped;\n",
    "    -- Error: 'case' is a nested object, use json_extract or UNNEST first\n",
    "\n",
    "    SELECT a.name, c.total_price FROM activities AS a, cases AS c;\n",
    "    -- Error: Cartesian join without ON condition\n",
    "\n",
    "    *Correct:*\n",
    "    SELECT a.name, c.total_price\n",
    "    FROM activities AS a\n",
    "    JOIN cases AS c ON a.case_id = c.id;\n",
    "\n",
    "    ###Output\n",
    "    Return only the SQL query. No markdown, no tags, no explanation.\n",
    "    Never guess values. Infer only from schema and question.\n",
    "    \"\"\"\n",
    "\n",
    "p2_p= \"\"\"/no_think \n",
    "      ### Database Schema\n",
    "\n",
    "                - **cases**  \n",
    "        - id (VARCHAR): Case identifier (PK)  \n",
    "        - avg_time (DOUBLE): Total duration (sec) from start to closure  \n",
    "        - type, branch, ramo, broker, state, client, creator (VARCHAR): Case metadata  \n",
    "        - value (BIGINT): Insurance amount  \n",
    "        - approved (BOOLEAN): Approval status  \n",
    "        - case_order_date, case_estimated_delivery, case_delivery (TIMESTAMP): Case timestamps  \n",
    "        - case_employee_id, case_branch, case_supplier (VARCHAR): Case-specific information  \n",
    "        - case_number_of_items, case_ft_items (INTEGER): Case item details  \n",
    "        - case_total_price (DOUBLE): Case total price\n",
    "\n",
    "        - **activities**  \n",
    "        - id (BIGINT): Activity identifier (PK)  \n",
    "        - case_id (VARCHAR): Case ID (FK → cases.id)  \n",
    "        - timestamp (TIMESTAMP): Activity timestamp  \n",
    "        - name (VARCHAR): Activity name  \n",
    "        - case_index (BIGINT): Alias of id  \n",
    "        - tpt (DOUBLE): Duration of the activity in seconds  \n",
    "        - user, user_type (VARCHAR): User-related info  \n",
    "        - automatic, rework (BOOLEAN): Activity flags  \n",
    "        - case_order_date (TIMESTAMP), case_employee_id (VARCHAR), case_branch (VARCHAR), case_supplier (VARCHAR): Case-related data  \n",
    "        - case_avg_time (DOUBLE): Average time for the case  \n",
    "        - case_on_time, case_in_full (BOOLEAN): Delivery status flags  \n",
    "        - case_number_of_items, case_ft_items (INTEGER): Case item counts  \n",
    "        - case_total_price (DOUBLE): Case total price  \n",
    "        - case_estimated_delivery, case_delivery (TIMESTAMP): Delivery-related timestamps\n",
    "\n",
    "        - **variants**  \n",
    "        - id (BIGINT): Variant ID (PK for path)  \n",
    "        - activities (VARCHAR[]): Ordered activity names for this path  \n",
    "        - cases (VARCHAR[]): IDs of cases that followed this path (→ cases.id)  \n",
    "        - number_cases (BIGINT): Total cases following this variant  \n",
    "        - percentage (DOUBLE): Percentage of total cases  \n",
    "        - avg_time (DOUBLE): Avg duration (sec) across cases in this variant\n",
    "\n",
    "            **Relations:**\n",
    "            - \"variants\".\"cases\" references \"cases\".\"id\", meaning each variant is followed by multiple cases.\n",
    "            - \"variants\".\"activities\" corresponds to the ordered \"activities\".\"name\" values for those cases.\n",
    "            \"\"\"\n",
    "p1_i= \"\"\" /no_think\n",
    "        You are an SQL assistant specialized in DuckDB. Your task is to generate accurate SQL queries based on natural language questions, following the schema and rules below.\n",
    "\n",
    "        ### Schema (Aliased)\n",
    "\n",
    "            - **grouped (g)**  \n",
    "            - group_id (VARCHAR): Unique identifier for each group (PK)  \n",
    "            - amount_overpaid (BIGINT): Total overpaid amount for the group  \n",
    "            - itemCount (BIGINT): Number of items in the group  \n",
    "            - date (VARCHAR): Date of the group  \n",
    "            - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "            - open (BOOLEAN): Status of the group (open or closed)  \n",
    "            - confidence (VARCHAR): Confidence level for detecting the pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "            - items (STRUCT[]): Array of items within the group, each containing:\n",
    "                - **id (INTEGER)**: Item identifier (FK → invoices.id)\n",
    "                - **case (STRUCT)**: Contains case details, such as:\n",
    "                    - id (VARCHAR): Case identifier  \n",
    "                    - order_date (VARCHAR): Order date for the case  \n",
    "                    - employee_id (VARCHAR): Employee ID handling the case  \n",
    "                    - branch (VARCHAR): Branch handling the case  \n",
    "                    - supplier (VARCHAR): Supplier associated with the case  \n",
    "                    - avg_time (DOUBLE): Average time for the case  \n",
    "                    - estimated_delivery (VARCHAR): Estimated delivery date for the case  \n",
    "                    - delivery (VARCHAR): Actual delivery date for the case  \n",
    "                    - on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "                    - in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "                    - number_of_items (INTEGER): Number of items in the case  \n",
    "                    - ft_items (INTEGER): Number of full-time items in the case  \n",
    "                    - total_price (INTEGER): Total price of the case  \n",
    "                - date (VARCHAR): Date of the item  \n",
    "                - unit_price (VARCHAR): Unit price of the item  \n",
    "                - quantity (INTEGER): Quantity of the item  \n",
    "                - value (VARCHAR): Value of the item  \n",
    "                - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'  \n",
    "                - open (BOOLEAN): Status of the item (open or closed)  \n",
    "                - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "                - confidence (VARCHAR): Confidence level for the item’s pattern (e.g., \"high\", \"medium\", \"low\")  \n",
    "                - description (VARCHAR): Description of the item  \n",
    "                - payment_method (VARCHAR): Payment method used for the item  \n",
    "                - pay_date (VARCHAR): Payment date of the item  \n",
    "                - special_instructions (VARCHAR): Special instructions for the item  \n",
    "                - accuracy (INTEGER): Accuracy of the item’s data matching\n",
    "\n",
    "            - **invoices (i)**  \n",
    "            - id (BIGINT): Invoice identifier (PK)  \n",
    "            - date (TIMESTAMP_NS): Date and time the invoice was issued  \n",
    "            - unit_price (VARCHAR): Unit price of the item in the invoice  \n",
    "            - quantity (BIGINT): Number of items in the invoice  \n",
    "            - value (VARCHAR): Total value of the invoice  \n",
    "            - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "            - open (BOOLEAN): Status of the invoice (open or closed)  \n",
    "            - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "            - confidence (VARCHAR): Confidence level for the invoice's pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "            - description (VARCHAR): Description of the invoice  \n",
    "            - payment_method (VARCHAR): Method used for payment  \n",
    "            - pay_date (TIMESTAMP_NS): Date and time the invoice was paid  \n",
    "            - special_instructions (VARCHAR): Any special instructions for the invoice  \n",
    "            - accuracy (BIGINT): Accuracy of the invoice's data matching  \n",
    "            - case_id (VARCHAR): Case identifier associated with the invoice  \n",
    "            - case_order_date (TIMESTAMP_NS): Order date of the case  \n",
    "            - case_employee_id (VARCHAR): Employee associated with the case  \n",
    "            - case_branch (VARCHAR): Branch where the case was handled  \n",
    "            - case_supplier (VARCHAR): Supplier associated with the case  \n",
    "            - case_avg_time (DOUBLE): Average time for the case  \n",
    "            - case_estimated_delivery (TIMESTAMP_NS): Estimated delivery date for the case  \n",
    "            - case_delivery (TIMESTAMP_NS): Actual delivery date for the case  \n",
    "            - case_on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "            - case_in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "            - case_number_of_items (BIGINT): Number of items in the case  \n",
    "            - case_ft_items (BIGINT): Number of full-time items in the case  \n",
    "            - case_total_price (BIGINT): Total price of the case\n",
    "\n",
    "        ### Query Guidelines\n",
    "\n",
    "        1. **Prefer Direct Tables**:  \n",
    "        Use `grouped (g)` or `invoices (i)` directly unless item-level fields are explicitly needed.\n",
    "\n",
    "        2. **UNNEST Only When Necessary**:\n",
    "        - Only use `UNNEST(g.items) AS item` when accessing nested fields (e.g., `item.case.supplier`, `item.unit_price`, etc.)\n",
    "        - After unnesting, access fields as `item.field` or `item.case.supplier`, **not** `item.unnest.field`.\n",
    "\n",
    "        3. **Nesting and Access Rules**:\n",
    "        - To access supplier from `grouped`, unnest items and use:  \n",
    "            ```sql\n",
    "            FROM grouped g, UNNEST(g.items) AS item\n",
    "            WHERE item.case.supplier = 'Example'\n",
    "            ```\n",
    "        - Avoid referencing nested fields without unnesting first.\n",
    "\n",
    "        4. **Case Sensitivity**:\n",
    "        - Use exact case for values:\n",
    "            - Confidence: 'High', 'Medium', 'Low'\n",
    "            - Pattern: 'Similar Value', 'Similar Reference', 'Exact Match', 'Similar Date', 'Similar Vendor', 'Multiple'\n",
    "\n",
    "        5. **Use Table Aliases**:\n",
    "        - Always use `g.` for `grouped`, `i.` for `invoices`, and `item.` after unnesting.\n",
    "\n",
    "        6. **Use TRIM() for Comparisons**:\n",
    "        - For text comparisons like pattern or supplier, wrap with `TRIM()`.  \n",
    "            Example: `TRIM(item.case.supplier) = 'VendorName'`\n",
    "\n",
    "        7. **Use IN / = ANY for Multiple Matches**:\n",
    "        - Use `pattern = ANY (['Value1', 'Value2'])` or `IN (...)` instead of OR chains.\n",
    "\n",
    "        8. **GROUP BY Nested Fields**:\n",
    "        - If grouping by nested fields like supplier, first unnest, then group by `item.case.supplier`.\n",
    "\n",
    "        9. **Aggregation and Filtering**:\n",
    "        - Use `ORDER BY ... LIMIT 1` instead of `> ALL(...)`\n",
    "        - Filter early with WHERE clauses to improve performance.\n",
    "\n",
    "        10. **Alternative Access**:\n",
    "        - Use `invoices` for simpler flat queries (e.g., `i.case_supplier`).\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Output Rules\n",
    "\n",
    "        - ❌ Do NOT explain the query.\n",
    "        - ✅ Only return the SQL query (no markdown, no comments, no formatting).\n",
    "        - ❌ Do NOT guess field names.\n",
    "        - ✅ Always respect the provided schema and capitalization.\n",
    "        \"\"\"\n",
    "\n",
    "p2_i= \"\"\" /no_think\n",
    "    ### Schema (Aliased)\n",
    "\n",
    "    - **grouped (g)**  \n",
    "    - group_id (VARCHAR): Unique identifier for each group (PK)  \n",
    "    - amount_overpaid (BIGINT): Total overpaid amount for the group  \n",
    "    - itemCount (BIGINT): Number of items in the group  \n",
    "    - date (VARCHAR): Date of the group  \n",
    "    - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "    - open (BOOLEAN): Status of the group (open or closed)  \n",
    "    - confidence (VARCHAR): Confidence level for detecting the pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "    - items (STRUCT[]): Array of items within the group, each containing:\n",
    "        - **id (INTEGER)**: Item identifier (FK → invoices.id)\n",
    "        - **case (STRUCT)**: Contains case details, such as:\n",
    "            - id (VARCHAR): Case identifier  \n",
    "            - order_date (VARCHAR): Order date for the case  \n",
    "            - employee_id (VARCHAR): Employee ID handling the case  \n",
    "            - branch (VARCHAR): Branch handling the case  \n",
    "            - supplier (VARCHAR): Supplier associated with the case  \n",
    "            - avg_time (DOUBLE): Average time for the case  \n",
    "            - estimated_delivery (VARCHAR): Estimated delivery date for the case  \n",
    "            - delivery (VARCHAR): Actual delivery date for the case  \n",
    "            - on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "            - in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "            - number_of_items (INTEGER): Number of items in the case  \n",
    "            - ft_items (INTEGER): Number of full-time items in the case  \n",
    "            - total_price (INTEGER): Total price of the case  \n",
    "        - date (VARCHAR): Date of the item  \n",
    "        - unit_price (VARCHAR): Unit price of the item  \n",
    "        - quantity (INTEGER): Quantity of the item  \n",
    "        - value (VARCHAR): Value of the item  \n",
    "        - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'  \n",
    "        - open (BOOLEAN): Status of the item (open or closed)  \n",
    "        - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "        - confidence (VARCHAR): Confidence level for the item’s pattern (e.g., \"high\", \"medium\", \"low\")  \n",
    "        - description (VARCHAR): Description of the item  \n",
    "        - payment_method (VARCHAR): Payment method used for the item  \n",
    "        - pay_date (VARCHAR): Payment date of the item  \n",
    "        - special_instructions (VARCHAR): Special instructions for the item  \n",
    "        - accuracy (INTEGER): Accuracy of the item’s data matching\n",
    "\n",
    "    - **invoices (i)**  \n",
    "    - id (BIGINT): Invoice identifier (PK)  \n",
    "    - date (TIMESTAMP_NS): Date and time the invoice was issued  \n",
    "    - unit_price (VARCHAR): Unit price of the item in the invoice  \n",
    "    - quantity (BIGINT): Number of items in the invoice  \n",
    "    - value (VARCHAR): Total value of the invoice  \n",
    "    - pattern (VARCHAR): Pattern type for the group 'Similar Value','Similar Reference','Exact Match','Similar Date','Similar Vendor','Multiple'\n",
    "    - open (BOOLEAN): Status of the invoice (open or closed)  \n",
    "    - group_id (VARCHAR): Group identifier (FK → grouped.group_id)  \n",
    "    - confidence (VARCHAR): Confidence level for the invoice's pattern (e.g., \"High\", \"Medium\", \"Low\")  \n",
    "    - description (VARCHAR): Description of the invoice  \n",
    "    - payment_method (VARCHAR): Method used for payment  \n",
    "    - pay_date (TIMESTAMP_NS): Date and time the invoice was paid  \n",
    "    - special_instructions (VARCHAR): Any special instructions for the invoice  \n",
    "    - accuracy (BIGINT): Accuracy of the invoice's data matching  \n",
    "    - case_id (VARCHAR): Case identifier associated with the invoice  \n",
    "    - case_order_date (TIMESTAMP_NS): Order date of the case  \n",
    "    - case_employee_id (VARCHAR): Employee associated with the case  \n",
    "    - case_branch (VARCHAR): Branch where the case was handled  \n",
    "    - case_supplier (VARCHAR): Supplier associated with the case  \n",
    "    - case_avg_time (DOUBLE): Average time for the case  \n",
    "    - case_estimated_delivery (TIMESTAMP_NS): Estimated delivery date for the case  \n",
    "    - case_delivery (TIMESTAMP_NS): Actual delivery date for the case  \n",
    "    - case_on_time (BOOLEAN): Whether the case was delivered on time  \n",
    "    - case_in_full (BOOLEAN): Whether the case was delivered in full  \n",
    "    - case_number_of_items (BIGINT): Number of items in the case  \n",
    "    - case_ft_items (BIGINT): Number of full-time items in the case  \n",
    "    - case_total_price (BIGINT): Total price of the case\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompts_sql_generation= {\"0\":[p1_p,p2_p],\n",
    "            \"1\":[p1_i,p2_i]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f876e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Orchestrator nodes ==\n",
    "def planner_node(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        user_question = state[\"question\"]\n",
    "\n",
    "        plan_prompt = \"\"\" /no_think\n",
    "\n",
    "        Generate a numbered list of up to 10 sequential tasks needed to fully answer the user's question.\n",
    "        \n",
    "        You have access to two tools:\n",
    "        - [SQL,0]: Process Mining\n",
    "        - [SQL,1]: Invoice Analysis\n",
    "        - [THINK]: Reasoning/Interpretation\n",
    "        \n",
    "        Each task should include:\n",
    "        - A \"type\" field specifying the tool to use.\n",
    "        - A \"description\" of what the task will do.\n",
    "        - A \"reason\" explaining why this task is necessary.\n",
    "        - A \"steps\" field that lists the numbers of prior activities whose outputs are required to complete this task. If the task does not depend on any previous output, use an empty list.\n",
    "        \n",
    "        Format your output as a JSON object like:\n",
    "        {{\n",
    "            \"ACTIVITY1\": {{\n",
    "                \"type\": \"[SQL,0]\",\n",
    "                \"description\": \"...\",\n",
    "                \"reason\": \"...\",\n",
    "                \"steps\": []\n",
    "            }},\n",
    "            \"ACTIVITY2\": {{\n",
    "                \"type\": \"[THINK]\",\n",
    "                \"description\": \"...\",\n",
    "                \"reason\": \"...\",\n",
    "                \"steps\": [1]\n",
    "            }},\n",
    "            ...\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        llm = OllamaLLM(model=\"qwen3:8b\", temperature=0.0, enable_thinking=False)\n",
    "        planner = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", plan_prompt),\n",
    "            (\"human\", \"user question: {task}\"),\n",
    "        ]) | llm | StrOutputParser()\n",
    "\n",
    "        raw_plan = planner.invoke({\"task\": user_question})\n",
    "        print(raw_plan)\n",
    "\n",
    "        # Parse JSON-like plan\n",
    "        steps = []\n",
    "        pattern = re.compile(r'\"?(ACTIVITY\\d+)\"?\\s*:\\s*{')\n",
    "        lines = raw_plan.strip().splitlines()\n",
    "        current_step = None\n",
    "        #use_case = \"0\"\n",
    "\n",
    "        for line in lines:\n",
    "            match = pattern.match(line.strip())\n",
    "            if match:\n",
    "                if current_step:\n",
    "                    current_step.setdefault(\"type\", \"[THINK]\")\n",
    "                    current_step.setdefault(\"description\", \"\")\n",
    "                    current_step.setdefault(\"reason\", \"\")\n",
    "                    current_step.setdefault(\"steps\", [])\n",
    "                    steps.append(current_step)\n",
    "                current_step = {\"id\": match.group(1)}\n",
    "            elif current_step:\n",
    "                if '\"type\"' in line:\n",
    "                    task_type = re.search(r'\"type\"\\s*:\\s*\"([^\"]+)\",?', line)\n",
    "                    if task_type:\n",
    "                        current_step[\"type\"] = task_type.group(1)\n",
    "                        if \"[SQL,1]\" in task_type.group(1):\n",
    "                            use_case = \"1\"\n",
    "                elif '\"description\"' in line:\n",
    "                    desc = re.search(r'\"description\"\\s*:\\s*\"([^\"]+)\",?', line)\n",
    "                    if desc:\n",
    "                        current_step[\"description\"] = desc.group(1)\n",
    "                elif '\"reason\"' in line:\n",
    "                    reason = re.search(r'\"reason\"\\s*:\\s*\"([^\"]+)\",?', line)\n",
    "                    if reason:\n",
    "                        current_step[\"reason\"] = reason.group(1)\n",
    "                elif '\"steps\"' in line:\n",
    "                    steps_str = re.search(r'\"steps\"\\s*:\\s*\\[([^\\]]*)\\]', line)\n",
    "                    if steps_str:\n",
    "                        current_step[\"steps\"] = [int(x.strip())-1 for x in steps_str.group(1).split(\",\") if x.strip()]\n",
    "\n",
    "        if current_step:\n",
    "            current_step.setdefault(\"type\", \"[THINK]\")\n",
    "            current_step.setdefault(\"description\", \"\")\n",
    "            current_step.setdefault(\"reason\", \"\")\n",
    "            current_step.setdefault(\"steps\", [])\n",
    "            steps.append(current_step)\n",
    "\n",
    "        return {\n",
    "            \"plan\": steps,\n",
    "            \"current_step\": 0, # Start from the first step\n",
    "            \"results\": {},\n",
    "            \"query_results\": [],\n",
    "            \"db_conn\": db_conn,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"use_case\": use_case,\n",
    "            \"question\": user_question\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in planner_node: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def execute_task_node(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        step = state[\"plan\"][state[\"current_step\"]]\n",
    "        task = step[\"description\"]\n",
    "        dependencies = step[\"steps\"]\n",
    "        logger.info(f\"Previous steps: {dependencies}\")\n",
    "        task_type = step[\"type\"]\n",
    "        # dependencies = step[\"steps\"] # Not used in this version. Usar en context con if step in dependencies\n",
    "\n",
    "        context = \"\\n\".join(f\"[Step {step}] {state['results'][step]}\" for step in sorted(state[\"results\"], key=int) if int(step) in dependencies)\n",
    "        logger.info(f\"Context: {context}\")\n",
    "\n",
    "        print(f\"\\n[Task {state['current_step'] + 1}] {task}\")\n",
    "\n",
    "        if \"SQL\" in task_type:\n",
    "            use_case = state[\"use_case\"]\n",
    "            system_prompt, repair_prompt = prompts_sql_generation[use_case]\n",
    "            answer, raw_result = run_sql_workflow(\n",
    "                task, state[\"db_conn\"], use_case, state[\"tokenizer\"], context, system_prompt, repair_prompt\n",
    "            )\n",
    "        else:\n",
    "            answer = run_think_task(task, context)\n",
    "            raw_result = answer\n",
    "\n",
    "        return {\n",
    "            \"plan\": state[\"plan\"],\n",
    "            \"results\": {**state[\"results\"],str(state[\"current_step\"]): answer}, #Saves answer before updating the current step\n",
    "            \"current_step\": state[\"current_step\"] + 1,            \n",
    "            \"query_results\": state[\"query_results\"] + [raw_result],\n",
    "            \"db_conn\": state[\"db_conn\"],\n",
    "            \"tokenizer\": state[\"tokenizer\"],\n",
    "            \"use_case\": state[\"use_case\"],\n",
    "            \"question\": state[\"question\"]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in execute_task_node: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7333967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Orchestrator routers ===\n",
    "def node_router(state: AgentState) -> str:\n",
    "    try:\n",
    "        next_node =  END if state[\"current_step\"] >= len(state[\"plan\"]) else \"execute_task\"\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in node_router: {e}\")\n",
    "    return next_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a49066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Orchetrator workflow ===\n",
    "def build_orchestrator_workflow():\n",
    "    try:\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"planner\", planner_node)\n",
    "        graph.add_node(\"execute_task\", execute_task_node)\n",
    "        graph.set_entry_point(\"planner\")\n",
    "        graph.add_edge(\"planner\", \"execute_task\")\n",
    "        graph.add_conditional_edges(\"execute_task\", node_router)\n",
    "        graph.set_finish_point(\"execute_task\")\n",
    "        return graph.compile()\n",
    "    except Exception as e:\n",
    "        logger.exception(f\" Error compiling Orchestrator workflow: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af993b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{\n",
      "    \"ACTIVITY1\": {\n",
      "        \"type\": \"[SQL,1]\",\n",
      "        \"description\": \"Retrieve all invoice data including invoice IDs and amounts.\",\n",
      "        \"reason\": \"To identify potential duplicates, we need the full invoice dataset.\",\n",
      "        \"steps\": []\n",
      "    },\n",
      "    \"ACTIVITY2\": {\n",
      "        \"type\": \"[THINK]\",\n",
      "        \"description\": \"Analyze invoice data to detect duplicates based on invoice ID and amount.\",\n",
      "        \"reason\": \"Duplicates can be identified by comparing invoice IDs and amounts across the dataset.\",\n",
      "        \"steps\": [1]\n",
      "    },\n",
      "    \"ACTIVITY3\": {\n",
      "        \"type\": \"[SQL,1]\",\n",
      "        \"description\": \"Count the number of invoices that have been identified as duplicates.\",\n",
      "        \"reason\": \"This provides the final count of duplicated invoices.\",\n",
      "        \"steps\": [2]\n",
      "    }\n",
      "}\n",
      "\n",
      "[Task 1] Retrieve all invoice data including invoice IDs and amounts.\n",
      "Exitoso el use case <think>\n",
      "Okay, let's see. The user wants to retrieve all invoice data, including invoice IDs and amounts. The options are Process Mining (0) or Invoice Analysis (1). \n",
      "\n",
      "First, I need to understand what each use case entails. Process Mining typically involves analyzing process data to discover, monitor, and improve business processes. It might look at things like workflow steps, timestamps, and activities.\n",
      "\n",
      "Invoice Analysis, on the other hand, would focus on examining invoice data for details like amounts, IDs, dates, and maybe verifying accuracy or processing payments. The question specifically mentions retrieving invoice data with IDs and amounts, which sounds like extracting information from invoices. \n",
      "\n",
      "So, the user is asking to get data from invoices, which aligns more with Invoice Analysis. Process Mining might involve analyzing the process of how invoices are handled, but the question is about retrieving the data itself. Therefore, the correct classification should be 1.\n",
      "</think>\n",
      "\n",
      "1\n",
      "Converting question to SQL: Retrieve all invoice data including invoice IDs and amounts.\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT i.id, i.value FROM invoices i;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT i.id, i.value FROM invoices i;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "\n",
      "[Task 2] Analyze invoice data to detect duplicates based on invoice ID and amount.\n",
      "\n",
      "[Task 3] Count the number of invoices that have been identified as duplicates.\n",
      "Exitoso el use case <think>\n",
      "Okay, let's see. The user is asking to count the number of invoices identified as duplicates. So first, I need to figure out which category this falls into. The options are Process Mining (0) or Invoice Analysis (1). \n",
      "\n",
      "Process Mining usually involves analyzing business processes using event logs, looking for bottlenecks or inefficiencies. Invoice Analysis, on the other hand, deals with processing invoices, checking for duplicates, errors, or discrepancies. The question is specifically about counting duplicate invoices, which is a common task in managing invoices. So this seems more related to Invoice Analysis. Therefore, the answer should be 1.\n",
      "</think>\n",
      "\n",
      "1\n",
      "Converting question to SQL: Count the number of invoices that have been identified as duplicates.\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT COUNT(*) FROM invoices i WHERE i.pattern = 'Exact Match' AND i.confidence = 'High'\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT COUNT(*) FROM invoices i WHERE i.pattern = 'Exact Match' AND i.confidence = 'High'\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n",
      "🚀 Executing query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "⚠️ Fixing SQL query: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "SELECT g.group_id, SUM(i.value) AS total_value, COUNT(*) AS item_count\n",
      "FROM grouped g\n",
      "JOIN invoices i ON g.group_id = i.group_id\n",
      "GROUP BY g.group_id;\n",
      "🔍 Error encountered: Error executing SQL query: Parser Error: syntax error at or near \"<\"\n"
     ]
    }
   ],
   "source": [
    "workflow = build_orchestrator_workflow()\n",
    "output = workflow.invoke({\"question\": \"How many invoices are duplicated?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab5e690-927e-4537-a7bd-83994b8c4bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': \"I couldn't generate a valid SQL query after 3 attempts. Please try again.\",\n",
       " '1': \"<think>\\n\\n</think>\\n\\nTo analyze invoice data for duplicates based on **invoice ID** and **amount**, you need to write an SQL query that identifies records with the same invoice ID and amount. Here's a corrected and valid SQL query to achieve this:\\n\\n```sql\\nSELECT invoice_id, amount, COUNT(*) AS duplicate_count\\nFROM invoices\\nGROUP BY invoice_id, amount\\nHAVING COUNT(*) > 1;\\n```\\n\\n### Explanation:\\n- **`GROUP BY invoice_id, amount`** groups the data by both fields.\\n- **`HAVING COUNT(*) > 1`** filters the results to only include groups that have more than one record, indicating potential duplicates.\\n\\nThis query will return all invoice IDs and amounts that appear more than once in the dataset, helping you identify possible duplicate entries.\",\n",
       " '2': \"I couldn't generate a valid SQL query after 3 attempts. Please try again.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832e692d-08ac-4acf-adea-dd18ecdbea00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many invoices are duplicated?',\n",
       " 'plan': [{'id': 'ACTIVITY1',\n",
       "   'type': '[SQL,1]',\n",
       "   'description': 'Retrieve all invoice data including invoice IDs and amounts.',\n",
       "   'reason': 'To identify potential duplicates, we need the full invoice dataset.',\n",
       "   'steps': []},\n",
       "  {'id': 'ACTIVITY2',\n",
       "   'type': '[THINK]',\n",
       "   'description': 'Analyze invoice data to detect duplicates based on invoice ID and amount.',\n",
       "   'reason': 'Duplicates can be identified by comparing invoice IDs and amounts across the dataset.',\n",
       "   'steps': [0]},\n",
       "  {'id': 'ACTIVITY3',\n",
       "   'type': '[SQL,1]',\n",
       "   'description': 'Count the number of invoices that have been identified as duplicates.',\n",
       "   'reason': 'This provides the final count of duplicated invoices.',\n",
       "   'steps': [1]}],\n",
       " 'current_step': 3,\n",
       " 'results': {'0': \"I couldn't generate a valid SQL query after 3 attempts. Please try again.\",\n",
       "  '1': \"<think>\\n\\n</think>\\n\\nTo analyze invoice data for duplicates based on **invoice ID** and **amount**, you need to write an SQL query that identifies records with the same invoice ID and amount. Here's a corrected and valid SQL query to achieve this:\\n\\n```sql\\nSELECT invoice_id, amount, COUNT(*) AS duplicate_count\\nFROM invoices\\nGROUP BY invoice_id, amount\\nHAVING COUNT(*) > 1;\\n```\\n\\n### Explanation:\\n- **`GROUP BY invoice_id, amount`** groups the data by both fields.\\n- **`HAVING COUNT(*) > 1`** filters the results to only include groups that have more than one record, indicating potential duplicates.\\n\\nThis query will return all invoice IDs and amounts that appear more than once in the dataset, helping you identify possible duplicate entries.\",\n",
       "  '2': \"I couldn't generate a valid SQL query after 3 attempts. Please try again.\"},\n",
       " 'query_results': ['Please try again.',\n",
       "  \"<think>\\n\\n</think>\\n\\nTo analyze invoice data for duplicates based on **invoice ID** and **amount**, you need to write an SQL query that identifies records with the same invoice ID and amount. Here's a corrected and valid SQL query to achieve this:\\n\\n```sql\\nSELECT invoice_id, amount, COUNT(*) AS duplicate_count\\nFROM invoices\\nGROUP BY invoice_id, amount\\nHAVING COUNT(*) > 1;\\n```\\n\\n### Explanation:\\n- **`GROUP BY invoice_id, amount`** groups the data by both fields.\\n- **`HAVING COUNT(*) > 1`** filters the results to only include groups that have more than one record, indicating potential duplicates.\\n\\nThis query will return all invoice IDs and amounts that appear more than once in the dataset, helping you identify possible duplicate entries.\",\n",
       "  'Please try again.'],\n",
       " 'db_conn': <duckdb.duckdb.DuckDBPyConnection at 0x78366170f970>,\n",
       " 'tokenizer': LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " ),\n",
       " 'use_case': '1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb21609-1931-4cf0-8af2-34400657eb56",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'final_answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_answer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'final_answer'"
     ]
    }
   ],
   "source": [
    "output['final_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a13377-c43b-4237-b197-7b4915654c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
